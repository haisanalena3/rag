import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import hashlib
import json
import time
from datetime import datetime
from pathlib import Path
from PIL import Image
import io
import traceback
import shutil
from config_local import load_config_local

class WebScraperLocal:
    def __init__(self, save_dir="db", overwrite=True):
        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n db
        current_dir = Path(__file__).parent
        if current_dir.name == "src":
            project_root = current_dir.parent
            self.save_dir = project_root / save_dir
        else:
            self.save_dir = Path(save_dir)

        # X·ª≠ l√Ω ghi ƒë√® database
        if overwrite and self.save_dir.exists():
            print(f"üóëÔ∏è ƒêang x√≥a database c≈©: {self.save_dir}")
            shutil.rmtree(self.save_dir, ignore_errors=True)
            print(f"‚úÖ ƒê√£ x√≥a database c≈©")

        self.save_dir.mkdir(exist_ok=True)
        print(f"üìÅ T·∫°o th∆∞ m·ª•c database: {self.save_dir.absolute()}")

        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        self.success_count = 0
        self.error_count = 0

    def clean_filename(self, url):
        """T·∫°o t√™n file an to√†n t·ª´ URL"""
        parsed = urlparse(url)
        filename = f"{parsed.netloc}_{parsed.path}".replace("/", "_").replace(".", "_")
        filename = "".join(c for c in filename if c.isalnum() or c in "._-")
        return filename[:100]

    def extract_metadata(self, soup, url):
        """Tr√≠ch xu·∫•t metadata t·ª´ HTML"""
        title = ""
        if soup.title:
            title = soup.title.string.strip()
        
        description = ""
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc:
            description = meta_desc.get("content", "")
        
        if not description:
            meta_desc = soup.find("meta", attrs={"property": "og:description"})
            if meta_desc:
                description = meta_desc.get("content", "")

        return {
            "title": title or f"Trang web t·ª´ {urlparse(url).netloc}",
            "description": description or "Kh√¥ng c√≥ m√¥ t·∫£",
            "url": url,
            "scraped_at": datetime.now().isoformat()
        }

    def clean_text(self, soup):
        """Tr√≠ch xu·∫•t v√† l√†m s·∫°ch text t·ª´ HTML"""
        # X√≥a script v√† style
        for script in soup(["script", "style"]):
            script.decompose()

        # L·∫•y text
        text = soup.get_text()
        
        # L√†m s·∫°ch
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text

    def get_image_info(self, img_tag, base_url):
        """L·∫•y th√¥ng tin ·∫£nh t·ª´ img tag"""
        src = img_tag.get('src') or img_tag.get('data-src')
        if not src:
            return None

        # T·∫°o URL ƒë·∫ßy ƒë·ªß
        img_url = urljoin(base_url, src)
        
        return {
            'url': img_url,
            'alt': img_tag.get('alt', ''),
            'title': img_tag.get('title', ''),
            'width': img_tag.get('width', ''),
            'height': img_tag.get('height', '')
        }

    def fetch_and_save(self, url):
        """T·∫£i v√† l∆∞u n·ªôi dung t·ª´ URL v·ªõi ghi ƒë√®"""
        try:
            print(f"\nüåê ƒêang x·ª≠ l√Ω: {url}")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', '').lower()
            if not content_type.startswith('text/html'):
                print(f"‚ö†Ô∏è Kh√¥ng ph·∫£i trang HTML: {content_type}")
                return False

            soup = BeautifulSoup(response.text, "html.parser")
            
            base_filename = self.clean_filename(url)
            page_dir = self.save_dir / base_filename

            # X√≥a th∆∞ m·ª•c c≈© n·∫øu t·ªìn t·∫°i
            if page_dir.exists():
                print(f"üóëÔ∏è X√≥a d·ªØ li·ªáu c≈©: {page_dir.name}")
                shutil.rmtree(page_dir, ignore_errors=True)

            page_dir.mkdir(exist_ok=True)
            print(f"üìÅ T·∫°o th∆∞ m·ª•c m·ªõi: {page_dir.name}")

            metadata = self.extract_metadata(soup, url)
            print(f"üìã Title: {metadata['title']}")

            clean_text = self.clean_text(soup)
            if not clean_text.strip():
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung text")
                clean_text = f"N·ªôi dung t·ª´ {url}\nKh√¥ng th·ªÉ tr√≠ch xu·∫•t text t·ª´ trang n√†y."

            text_file = page_dir / "content.txt"
            with open(text_file, "w", encoding="utf-8") as f:
                f.write(clean_text)
            print(f"üíæ L∆∞u text: {len(clean_text)} k√Ω t·ª±")

            img_tags = soup.find_all("img")
            img_dir = page_dir / "images"
            img_dir.mkdir(exist_ok=True)
            print(f"üñºÔ∏è T√¨m th·∫•y {len(img_tags)} img tags")

            downloaded_images = []
            unique_urls = set()

            for idx, img_tag in enumerate(img_tags):
                img_info = self.get_image_info(img_tag, url)
                if not img_info or img_info['url'] in unique_urls:
                    continue

                unique_urls.add(img_info['url'])
                
                img_path = self.download_image(
                    img_info['url'],
                    img_dir,
                    base_filename,
                    idx
                )

                if img_path:
                    img_info['local_path'] = img_path
                    downloaded_images.append(img_info)

                time.sleep(0.2)

            metadata['image_count'] = len(downloaded_images)

            # ƒê·∫£m b·∫£o metadata c√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin
            required_fields = ['title', 'description', 'url', 'scraped_at']
            for field in required_fields:
                if field not in metadata or not metadata[field]:
                    if field == 'title':
                        metadata[field] = f"Trang web t·ª´ {urlparse(url).netloc}"
                    elif field == 'description':
                        metadata[field] = "Kh√¥ng c√≥ m√¥ t·∫£"
                    elif field == 'url':
                        metadata[field] = url
                    elif field == 'scraped_at':
                        metadata[field] = datetime.now().isoformat()

            metadata_file = page_dir / "metadata.json"
            metadata_content = {
                'metadata': metadata,
                'images': downloaded_images
            }

            # Ghi ƒë√® file metadata
            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata_content, f, ensure_ascii=False, indent=2)

            if not metadata_file.exists():
                print("‚ùå L·ªói: metadata.json kh√¥ng ƒë∆∞·ª£c t·∫°o")
                return False

            print(f"üíæ L∆∞u metadata: {metadata_file.name}")
            print(f"‚úÖ Ho√†n th√†nh {url}")
            print(f"   üìÑ Text: {text_file.name}")
            print(f"   üñºÔ∏è ·∫¢nh: {len(downloaded_images)} files")
            print(f"   üìã Metadata: {metadata_file.name}")

            self.success_count += 1
            return True

        except requests.exceptions.RequestException as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi {url}: {e}")
            self.error_count += 1
            return False
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω {url}: {e}")
            print("Chi ti·∫øt l·ªói:")
            traceback.print_exc()
            self.error_count += 1
            return False

    def download_image(self, img_url, img_dir, filename_prefix, index):
        """T·∫£i v√† l∆∞u ·∫£nh v·ªõi ghi ƒë√®"""
        try:
            print(f"   ƒêang t·∫£i ·∫£nh {index + 1}: {img_url}")
            
            response = self.session.get(img_url, timeout=15, stream=True)
            response.raise_for_status()

            content_type = response.headers.get('content-type', '').lower()
            if not content_type.startswith('image/'):
                print(f"   ‚ö†Ô∏è Kh√¥ng ph·∫£i file ·∫£nh: {content_type}")
                return None

            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > 10 * 1024 * 1024:
                print(f"   ‚ö†Ô∏è File qu√° l·ªõn: {content_length} bytes")
                return None

            content = response.content
            if len(content) < 100:
                print(f"   ‚ö†Ô∏è File qu√° nh·ªè: {len(content)} bytes")
                return None

            img_hash = hashlib.md5(content).hexdigest()[:10]

            ext_map = {
                'image/jpeg': '.jpg',
                'image/jpg': '.jpg',
                'image/png': '.png',
                'image/gif': '.gif',
                'image/webp': '.webp',
                'image/bmp': '.bmp',
                'image/svg+xml': '.svg',
                'image/x-icon': '.ico',
                'image/vnd.microsoft.icon': '.ico'
            }

            ext = ext_map.get(content_type)
            if not ext:
                parsed = urlparse(img_url)
                ext = os.path.splitext(parsed.path)[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg', '.ico']:
                    ext = '.jpg'

            filename = f"{filename_prefix}_img_{index}_{img_hash}{ext}"
            img_path = img_dir / filename

            # Ghi ƒë√® ·∫£nh n·∫øu ƒë√£ t·ªìn t·∫°i
            try:
                if ext not in ['.svg', '.ico']:
                    img = Image.open(io.BytesIO(content))
                    width, height = img.size
                    if width < 50 or height < 50:
                        print(f"   ‚ö†Ô∏è ·∫¢nh qu√° nh·ªè: {width}x{height}")
                        return None

                    img.verify()
                    img = Image.open(io.BytesIO(content))
                    if img.mode in ('RGBA', 'LA', 'P'):
                        img = img.convert('RGB')
                    img.save(img_path, quality=85, optimize=True)
                else:
                    with open(img_path, 'wb') as f:
                        f.write(content)

                print(f"   ‚úÖ ƒê√£ l∆∞u: {filename}")
                return str(img_path)

            except Exception as e:
                try:
                    with open(img_path, 'wb') as f:
                        f.write(content)
                    print(f"   ‚úÖ ƒê√£ l∆∞u (raw): {filename}")
                    return str(img_path)
                except Exception as e2:
                    print(f"   ‚ùå L·ªói l∆∞u file: {e2}")
                    return None

        except Exception as e:
            print(f"   ‚ùå L·ªói t·∫£i ·∫£nh: {e}")
            return None

    def print_summary(self):
        """In t√≥m t·∫Øt k·∫øt qu·∫£"""
        print(f"\n{'='*50}")
        print(f"üìä T√ìM T·∫ÆT K·∫æT QU·∫¢")
        print(f"{'='*50}")
        print(f"‚úÖ Th√†nh c√¥ng: {self.success_count}")
        print(f"‚ùå L·ªói: {self.error_count}")
        print(f"üìÅ D·ªØ li·ªáu l∆∞u t·∫°i: {self.save_dir.absolute()}")
        
        if self.save_dir.exists():
            subdirs = [d for d in self.save_dir.iterdir() if d.is_dir()]
            total_images = 0
            for subdir in subdirs:
                img_dir = subdir / "images"
                if img_dir.exists():
                    total_images += len(list(img_dir.glob("*")))
            print(f"üñºÔ∏è T·ªïng s·ªë ·∫£nh: {total_images}")
        
        print(f"{'='*50}")

def clear_database(db_dir):
    """X√≥a to√†n b·ªô database"""
    db_path = Path(db_dir)
    if db_path.exists():
        print(f"üóëÔ∏è ƒêang x√≥a to√†n b·ªô database: {db_path}")
        shutil.rmtree(db_path, ignore_errors=True)
        print(f"‚úÖ ƒê√£ x√≥a database")
    else:
        print(f"‚ÑπÔ∏è Database kh√¥ng t·ªìn t·∫°i: {db_path}")

def main():
    """H√†m ch√≠nh v·ªõi t√πy ch·ªçn ghi ƒë√®"""
    try:
        print("üöÄ B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu v·ªõi Gemma Local...")
        
        config = load_config_local()
        urls = config.get("URLS", [])
        
        if not urls:
            print("‚ùå Kh√¥ng t√¨m th·∫•y URLs trong config_local!")
            print("Vui l√≤ng ki·ªÉm tra file config_local.py v√† ƒë·∫£m b·∫£o c√≥ danh s√°ch URLS")
            return

        print(f"üìã T√¨m th·∫•y {len(urls)} URLs trong config")

        # H·ªèi ng∆∞·ªùi d√πng c√≥ mu·ªën ghi ƒë√® kh√¥ng
        overwrite_choice = input("\n‚ùì B·∫°n c√≥ mu·ªën ghi ƒë√® database c≈© kh√¥ng? (y/N): ").lower().strip()
        overwrite = overwrite_choice in ['y', 'yes', 'c√≥']

        if overwrite:
            print("üîÑ S·∫Ω ghi ƒë√® database c≈©")
        else:
            print("üìÇ S·∫Ω gi·ªØ l·∫°i d·ªØ li·ªáu c≈© v√† ch·ªâ c·∫≠p nh·∫≠t")

        for i, url in enumerate(urls, 1):
            print(f"  {i}. {url}")

        # Kh·ªüi t·∫°o scraper v·ªõi t√πy ch·ªçn ghi ƒë√®
        scraper = WebScraperLocal(overwrite=overwrite)

        print(f"\nüîÑ B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu t·ª´ {len(urls)} URLs...")

        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] X·ª≠ l√Ω URL: {url}")
            success = scraper.fetch_and_save(url)
            
            if i < len(urls):
                print("‚è≥ Ch·ªù 2 gi√¢y tr∆∞·ªõc khi x·ª≠ l√Ω URL ti·∫øp theo...")
                time.sleep(2)

        scraper.print_summary()

        if scraper.success_count > 0:
            print(f"\nüéâ Ho√†n th√†nh thu th·∫≠p d·ªØ li·ªáu!")
            print(f"B·∫°n c√≥ th·ªÉ ch·∫°y ·ª©ng d·ª•ng Streamlit ƒë·ªÉ xem k·∫øt qu·∫£:")
            print(f"streamlit run app_local.py")
        else:
            print(f"\nüòû Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu n√†o!")
            print(f"Vui l√≤ng ki·ªÉm tra:")
            print(f"- K·∫øt n·ªëi internet")
            print(f"- URLs trong config_local.py c√≥ h·ª£p l·ªá kh√¥ng")
            print(f"- C√°c website c√≥ th·ªÉ truy c·∫≠p ƒë∆∞·ª£c kh√¥ng")

    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è Ng∆∞·ªùi d√πng d·ª´ng ch∆∞∆°ng tr√¨nh")
    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh th·ª±c thi: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()

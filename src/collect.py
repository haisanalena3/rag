import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import hashlib
import json
import time
from datetime import datetime
from pathlib import Path
from PIL import Image
import io
import traceback
from config import load_config

class WebScraper:
    def __init__(self, save_dir="db"):
        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n db
        current_dir = Path(__file__).parent
        if current_dir.name == "src":
            # N·∫øu ƒëang ·ªü trong th∆∞ m·ª•c src, l√πi v·ªÅ th∆∞ m·ª•c cha
            project_root = current_dir.parent
            self.save_dir = project_root / save_dir
        else:
            # N·∫øu ƒëang ·ªü th∆∞ m·ª•c g·ªëc
            self.save_dir = Path(save_dir)
            
        self.save_dir.mkdir(exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.success_count = 0
        self.error_count = 0

    def is_valid_image_url(self, url):
        """Ki·ªÉm tra URL ·∫£nh h·ª£p l·ªá"""
        if not url or not url.startswith(('http://', 'https://')):
            return False
        
        invalid_patterns = ['data:', 'javascript:', 'mailto:', '#', 'tel:']
        if any(pattern in url.lower() for pattern in invalid_patterns):
            return False
            
        valid_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg', '.ico']
        parsed = urlparse(url)
        path = parsed.path.lower()
        
        if '.' in path:
            ext = os.path.splitext(path)[1]
            if ext and ext not in valid_extensions:
                return False
                
        return True

    def clean_filename(self, url):
        """T·∫°o t√™n file s·∫°ch t·ª´ URL"""
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        path = parsed.path.strip('/').replace('/', '_')
        
        if path:
            filename = f"{domain}_{path}"
        else:
            filename = domain
            
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '_')
            
        filename = ''.join(c for c in filename if c.isalnum() or c in '._-')
        return filename[:100]

    def extract_metadata(self, soup, url):
        """Tr√≠ch xu·∫•t metadata t·ª´ trang web"""
        metadata = {
            'url': url,
            'title': '',
            'description': '',
            'keywords': '',
            'author': '',
            'language': '',
            'scraped_at': datetime.now().isoformat(),
            'image_count': 0
        }
        
        # L·∫•y title
        title_tag = soup.find('title')
        if title_tag:
            metadata['title'] = title_tag.get_text().strip()
            
        # L·∫•y meta description
        desc_tag = soup.find('meta', attrs={'name': 'description'}) or \
                   soup.find('meta', attrs={'property': 'og:description'})
        if desc_tag:
            metadata['description'] = desc_tag.get('content', '').strip()
            
        # L·∫•y meta keywords
        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_tag:
            metadata['keywords'] = keywords_tag.get('content', '').strip()
            
        # L·∫•y author
        author_tag = soup.find('meta', attrs={'name': 'author'})
        if author_tag:
            metadata['author'] = author_tag.get('content', '').strip()
            
        # L·∫•y language
        lang_tag = soup.find('html')
        if lang_tag and lang_tag.get('lang'):
            metadata['language'] = lang_tag.get('lang')
            
        if not metadata['title']:
            metadata['title'] = f"Trang web t·ª´ {urlparse(url).netloc}"
            
        return metadata

    def clean_text(self, soup):
        """L√†m s·∫°ch v√† tr√≠ch xu·∫•t text t·ª´ HTML"""
        for element in soup(["script", "style", "nav", "footer", "header", "aside", "noscript"]):
            element.decompose()
            
        content_selectors = [
            'main', 'article', '[role="main"]', '.content', '.main-content',
            '.post-content', '.entry-content', '.article-content'
        ]
        
        main_content = None
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
                
        if main_content:
            text = main_content.get_text(separator="\n", strip=True)
        else:
            body = soup.find('body')
            if body:
                text = body.get_text(separator="\n", strip=True)
            else:
                text = soup.get_text(separator="\n", strip=True)
            
        lines = []
        for line in text.split('\n'):
            line = line.strip()
            if line and len(line) > 2:
                lines.append(line)
                
        return '\n'.join(lines)

    def get_image_info(self, img_tag, base_url):
        """Tr√≠ch xu·∫•t th√¥ng tin ·∫£nh t·ª´ img tag"""
        img_info = {}
        
        img_url = (img_tag.get('src') or 
                  img_tag.get('data-src') or 
                  img_tag.get('data-original') or
                  img_tag.get('data-lazy-src') or
                  img_tag.get('data-srcset'))
        
        if not img_url:
            return None
            
        img_url = urljoin(base_url, img_url)
        
        if not self.is_valid_image_url(img_url):
            return None
            
        img_info['url'] = img_url
        img_info['alt'] = img_tag.get('alt', '').strip()
        img_info['title'] = img_tag.get('title', '').strip()
        img_info['width'] = img_tag.get('width', '')
        img_info['height'] = img_tag.get('height', '')
        
        srcset = img_tag.get('srcset') or img_tag.get('data-srcset')
        if srcset:
            srcset_urls = []
            for item in srcset.split(','):
                parts = item.strip().split()
                if parts:
                    url_candidate = urljoin(base_url, parts[0])
                    if self.is_valid_image_url(url_candidate):
                        srcset_urls.append(url_candidate)
            if srcset_urls:
                img_info['url'] = srcset_urls[-1]
                
        return img_info

    def download_image(self, img_url, img_dir, filename_prefix, index):
        """T·∫£i v√† l∆∞u ·∫£nh"""
        try:
            print(f"    ƒêang t·∫£i ·∫£nh {index + 1}: {img_url}")
            
            response = self.session.get(img_url, timeout=15, stream=True)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', '').lower()
            if not content_type.startswith('image/'):
                print(f"    ‚ö†Ô∏è Kh√¥ng ph·∫£i file ·∫£nh: {content_type}")
                return None
                
            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > 10 * 1024 * 1024:
                print(f"    ‚ö†Ô∏è File qu√° l·ªõn: {content_length} bytes")
                return None
                
            content = response.content
            if len(content) < 100:
                print(f"    ‚ö†Ô∏è File qu√° nh·ªè: {len(content)} bytes")
                return None
                
            img_hash = hashlib.md5(content).hexdigest()[:10]
            
            ext_map = {
                'image/jpeg': '.jpg',
                'image/jpg': '.jpg',
                'image/png': '.png',
                'image/gif': '.gif',
                'image/webp': '.webp',
                'image/bmp': '.bmp',
                'image/svg+xml': '.svg',
                'image/x-icon': '.ico',
                'image/vnd.microsoft.icon': '.ico'
            }
            
            ext = ext_map.get(content_type)
            if not ext:
                parsed = urlparse(img_url)
                ext = os.path.splitext(parsed.path)[-1].lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg', '.ico']:
                    ext = '.jpg'
                    
            filename = f"{filename_prefix}_img_{index}_{img_hash}{ext}"
            img_path = img_dir / filename
            
            if img_path.exists():
                print(f"    ‚úÖ ·∫¢nh ƒë√£ t·ªìn t·∫°i: {filename}")
                return str(img_path)
                
            try:
                if ext not in ['.svg', '.ico']:
                    img = Image.open(io.BytesIO(content))
                    
                    width, height = img.size
                    if width < 50 or height < 50:
                        print(f"    ‚ö†Ô∏è ·∫¢nh qu√° nh·ªè: {width}x{height}")
                        return None
                        
                    img.verify()
                    
                    img = Image.open(io.BytesIO(content))
                    if img.mode in ('RGBA', 'LA', 'P'):
                        img = img.convert('RGB')
                    img.save(img_path, quality=85, optimize=True)
                else:
                    with open(img_path, 'wb') as f:
                        f.write(content)
                        
                print(f"    ‚úÖ ƒê√£ l∆∞u: {filename}")
                return str(img_path)
                
            except Exception as e:
                try:
                    with open(img_path, 'wb') as f:
                        f.write(content)
                    print(f"    ‚úÖ ƒê√£ l∆∞u (raw): {filename}")
                    return str(img_path)
                except Exception as e2:
                    print(f"    ‚ùå L·ªói l∆∞u file: {e2}")
                    return None
                
        except Exception as e:
            print(f"    ‚ùå L·ªói t·∫£i ·∫£nh: {e}")
            return None

    def fetch_and_save(self, url):
        """T·∫£i v√† l∆∞u n·ªôi dung t·ª´ URL"""
        try:
            print(f"\nüåê ƒêang x·ª≠ l√Ω: {url}")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', '').lower()
            if not content_type.startswith('text/html'):
                print(f"‚ö†Ô∏è Kh√¥ng ph·∫£i trang HTML: {content_type}")
                return False
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            base_filename = self.clean_filename(url)
            page_dir = self.save_dir / base_filename
            page_dir.mkdir(exist_ok=True)
            
            print(f"üìÅ T·∫°o th∆∞ m·ª•c: {page_dir.name}")
            
            metadata = self.extract_metadata(soup, url)
            print(f"üìã Title: {metadata['title']}")
            
            clean_text = self.clean_text(soup)
            if not clean_text.strip():
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung text")
                clean_text = f"N·ªôi dung t·ª´ {url}\nKh√¥ng th·ªÉ tr√≠ch xu·∫•t text t·ª´ trang n√†y."
                
            text_file = page_dir / "content.txt"
            with open(text_file, "w", encoding="utf-8") as f:
                f.write(clean_text)
            print(f"üíæ L∆∞u text: {len(clean_text)} k√Ω t·ª±")
                
            img_tags = soup.find_all("img")
            img_dir = page_dir / "images"
            img_dir.mkdir(exist_ok=True)
            
            print(f"üñºÔ∏è T√¨m th·∫•y {len(img_tags)} img tags")
            
            downloaded_images = []
            unique_urls = set()
            
            for idx, img_tag in enumerate(img_tags):
                img_info = self.get_image_info(img_tag, url)
                if not img_info or img_info['url'] in unique_urls:
                    continue
                    
                unique_urls.add(img_info['url'])
                
                img_path = self.download_image(
                    img_info['url'], 
                    img_dir, 
                    base_filename, 
                    idx
                )
                
                if img_path:
                    img_info['local_path'] = img_path
                    downloaded_images.append(img_info)
                    
                time.sleep(0.2)
                
            metadata['image_count'] = len(downloaded_images)
            
            required_fields = ['title', 'description', 'url', 'scraped_at']
            for field in required_fields:
                if field not in metadata or not metadata[field]:
                    if field == 'title':
                        metadata[field] = f"Trang web t·ª´ {urlparse(url).netloc}"
                    elif field == 'description':
                        metadata[field] = "Kh√¥ng c√≥ m√¥ t·∫£"
                    elif field == 'url':
                        metadata[field] = url
                    elif field == 'scraped_at':
                        metadata[field] = datetime.now().isoformat()
            
            metadata_file = page_dir / "metadata.json"
            metadata_content = {
                'metadata': metadata,
                'images': downloaded_images
            }
            
            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata_content, f, ensure_ascii=False, indent=2)
            
            if not metadata_file.exists():
                print("‚ùå L·ªói: metadata.json kh√¥ng ƒë∆∞·ª£c t·∫°o")
                return False
                
            print(f"üíæ L∆∞u metadata: {metadata_file.name}")
            print(f"‚úÖ Ho√†n th√†nh {url}")
            print(f"   üìÑ Text: {text_file.name}")
            print(f"   üñºÔ∏è ·∫¢nh: {len(downloaded_images)} files")
            print(f"   üìã Metadata: {metadata_file.name}")
            
            self.success_count += 1
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"‚ùå L·ªói k·∫øt n·ªëi {url}: {e}")
            self.error_count += 1
            return False
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω {url}: {e}")
            print("Chi ti·∫øt l·ªói:")
            traceback.print_exc()
            self.error_count += 1
            return False

    def print_summary(self):
        """In t√≥m t·∫Øt k·∫øt qu·∫£"""
        print(f"\n{'='*50}")
        print(f"üìä T√ìM T·∫ÆT K·∫æT QU·∫¢")
        print(f"{'='*50}")
        print(f"‚úÖ Th√†nh c√¥ng: {self.success_count}")
        print(f"‚ùå L·ªói: {self.error_count}")
        print(f"üìÅ D·ªØ li·ªáu l∆∞u t·∫°i: {self.save_dir.absolute()}")
        
        if self.save_dir.exists():
            subdirs = [d for d in self.save_dir.iterdir() if d.is_dir()]
            total_images = 0
            for subdir in subdirs:
                img_dir = subdir / "images"
                if img_dir.exists():
                    total_images += len(list(img_dir.glob("*")))
            print(f"üñºÔ∏è T·ªïng s·ªë ·∫£nh: {total_images}")
        print(f"{'='*50}")

def main():
    """H√†m ch√≠nh"""
    try:
        print("üöÄ B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu...")
        
        config = load_config()
        urls = config.get("URLS", [])
        
        if not urls:
            print("‚ùå Kh√¥ng t√¨m th·∫•y URLs trong config!")
            print("Vui l√≤ng ki·ªÉm tra file config.py v√† ƒë·∫£m b·∫£o c√≥ danh s√°ch URLS")
            return
            
        print(f"üìã T√¨m th·∫•y {len(urls)} URLs trong config")
        
        for i, url in enumerate(urls, 1):
            print(f"  {i}. {url}")
        
        scraper = WebScraper()
        
        print(f"\nüîÑ B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu t·ª´ {len(urls)} URLs...")
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] X·ª≠ l√Ω URL: {url}")
            
            success = scraper.fetch_and_save(url)
            
            if i < len(urls):
                print("‚è≥ Ch·ªù 2 gi√¢y tr∆∞·ªõc khi x·ª≠ l√Ω URL ti·∫øp theo...")
                time.sleep(2)
        
        scraper.print_summary()
        
        if scraper.success_count > 0:
            print(f"\nüéâ Ho√†n th√†nh thu th·∫≠p d·ªØ li·ªáu!")
            print(f"B·∫°n c√≥ th·ªÉ ch·∫°y ·ª©ng d·ª•ng Streamlit ƒë·ªÉ xem k·∫øt qu·∫£:")
            print(f"streamlit run app.py")
        else:
            print(f"\nüòû Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu n√†o!")
            print(f"Vui l√≤ng ki·ªÉm tra:")
            print(f"- K·∫øt n·ªëi internet")
            print(f"- URLs trong config.py c√≥ h·ª£p l·ªá kh√¥ng")
            print(f"- C√°c website c√≥ th·ªÉ truy c·∫≠p ƒë∆∞·ª£c kh√¥ng")
        
    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh th·ª±c thi: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()

import os
import json
import streamlit as st
from pathlib import Path
from PIL import Image
import base64
from io import BytesIO
import traceback
import numpy as np
import re
import logging
import time
import shutil
import requests

# Fix torch classes path issue with Streamlit on Python 3.13
try:
    import torch
    if not hasattr(torch.classes, '__path__'):
        torch.classes.__path__ = [os.path.join(torch.__path__[0], 'classes')]
except ImportError:
    pass
except Exception as e:
    print(f"Warning: Could not fix torch.classes path: {e}")

from config_local import load_config_local
from rag_local import (
    load_documents, search_documents, ask_local_model,
    search_documents_with_threshold, adaptive_threshold_search,
    enhanced_search_with_metadata
)
from collect_local import WebScraperLocal
from local_gemma import LocalGemmaClient

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedMultimodalRAGLocal:
    def __init__(self, db_dir):
        self.db_dir = Path(db_dir)
        self.documents = []
        self.has_data = False
        self.query_history = []
        self.load_multimodal_data()

    def load_multimodal_data(self):
        """Load d·ªØ li·ªáu ƒëa ph∆∞∆°ng ti·ªán t·ª´ database v·ªõi debug chi ti·∫øt"""
        self.documents = []
        logger.info(f"Ki·ªÉm tra th∆∞ m·ª•c db: {self.db_dir.absolute()}")
        
        if not self.db_dir.exists():
            logger.warning(f"Th∆∞ m·ª•c db kh√¥ng t·ªìn t·∫°i: {self.db_dir}")
            self.has_data = False
            return

        site_dirs = [d for d in self.db_dir.iterdir() if d.is_dir()]
        logger.info(f"T√¨m th·∫•y {len(site_dirs)} th∆∞ m·ª•c con trong db")

        for site_dir in site_dirs:
            logger.info(f"Ki·ªÉm tra th∆∞ m·ª•c: {site_dir.name}")
            metadata_file = site_dir / "metadata.json"
            
            if not metadata_file.exists():
                logger.warning(f"Thi·∫øu metadata.json trong {site_dir.name}")
                continue

            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                content_file = site_dir / "content.txt"
                text_content = ""
                if content_file.exists():
                    with open(content_file, 'r', encoding='utf-8') as f:
                        text_content = f.read()

                doc = {
                    'site_name': site_dir.name,
                    'url': data['metadata']['url'],
                    'title': data['metadata']['title'],
                    'description': data['metadata']['description'],
                    'text_content': text_content,
                    'images': data.get('images', []),
                    'image_dir': site_dir / "images",
                    'metadata': data['metadata']
                }

                self.documents.append(doc)
                logger.info(f"Th√™m document: {doc['title']}")

            except Exception as e:
                logger.error(f"L·ªói load d·ªØ li·ªáu t·ª´ {site_dir}: {e}")

        self.has_data = len(self.documents) > 0
        logger.info(f"T·ªïng c·ªông: {len(self.documents)} documents")

    def get_database_info(self):
        """L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ database"""
        db_info = {
            'total_documents': len(self.documents),
            'total_words': 0,
            'total_images': 0,
            'content_types': {},
            'keywords': set(),
            'titles': [],
            'urls': [],
            'sample_content': []
        }

        for doc in self.documents:
            # Th·ªëng k√™ c∆° b·∫£n
            word_count = doc['metadata'].get('word_count', 0)
            db_info['total_words'] += word_count
            db_info['total_images'] += len(doc['images'])
            
            # Content types
            content_type = doc['metadata'].get('content_type', 'unknown')
            db_info['content_types'][content_type] = db_info['content_types'].get(content_type, 0) + 1
            
            # Titles v√† URLs
            db_info['titles'].append(doc['title'])
            db_info['urls'].append(doc['url'])
            
            # Sample content (first 200 chars)
            if doc['text_content']:
                sample = doc['text_content'][:200] + "..."
                db_info['sample_content'].append({
                    'title': doc['title'],
                    'sample': sample
                })

        return db_info

    def enhanced_search_multimodal(self, query, threshold=0.05, top_k=3):
        """T√¨m ki·∫øm v·ªõi threshold th·∫•p h∆°n ƒë·ªÉ t√¨m ƒë∆∞·ª£c k·∫øt qu·∫£"""
        if not self.has_data:
            return [], 0

        # Th·ª≠ v·ªõi enhanced_search_with_metadata v·ªõi threshold th·∫•p
        relevant_docs, max_similarity = enhanced_search_with_metadata(
            query, self.documents, threshold, top_k
        )
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ v·ªõi threshold c·ª±c th·∫•p
        if not relevant_docs and threshold > 0.01:
            logger.info(f"Kh√¥ng t√¨m th·∫•y v·ªõi threshold {threshold}, th·ª≠ v·ªõi 0.01")
            relevant_docs, max_similarity = enhanced_search_with_metadata(
                query, self.documents, 0.01, top_k
            )
        
        # N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y, th·ª≠ t√¨m ki·∫øm t·ª´ng t·ª´
        if not relevant_docs:
            logger.info(f"Th·ª≠ t√¨m ki·∫øm t·ª´ng t·ª´ trong query: {query}")
            words = query.split()
            for word in words:
                if len(word) > 2:  # B·ªè qua t·ª´ qu√° ng·∫Øn
                    word_results, word_similarity = enhanced_search_with_metadata(
                        word, self.documents, 0.01, top_k
                    )
                    if word_results:
                        relevant_docs = word_results
                        max_similarity = word_similarity
                        logger.info(f"T√¨m th·∫•y k·∫øt qu·∫£ v·ªõi t·ª´: {word}")
                        break

        # L∆∞u l·ªãch s·ª≠
        self.query_history.append({
            'query': query,
            'similarity': max_similarity,
            'results_count': len(relevant_docs),
            'timestamp': time.time(),
            'threshold_used': threshold
        })

        return relevant_docs, max_similarity

    def get_relevant_images_for_context(self, relevant_docs, query, max_images=5):
        """L·∫•y ·∫£nh li√™n quan ƒë·ªÉ ƒë∆∞a v√†o context v·ªõi scoring c·∫£i ti·∫øn"""
        relevant_images = []
        query_lower = query.lower()
        query_words = set(query_lower.split())

        for doc in relevant_docs:
            for img_info in doc['images']:
                relevance_score = 0
                alt_text = img_info.get('alt', '').lower()
                title_text = img_info.get('title', '').lower()

                # T√≠nh ƒëi·ªÉm d·ª±a tr√™n t·ª´ kh√≥a
                alt_words = set(alt_text.split())
                title_words = set(title_text.split())

                # Exact word matches
                alt_matches = len(query_words.intersection(alt_words))
                title_matches = len(query_words.intersection(title_words))
                relevance_score += (alt_matches * 2) + (title_matches * 2)

                # Partial matches
                for word in query_words:
                    if len(word) > 3:
                        if word in alt_text:
                            relevance_score += 1
                        if word in title_text:
                            relevance_score += 1

                # Bonus ƒëi·ªÉm cho ·∫£nh c√≥ m√¥ t·∫£ chi ti·∫øt
                if len(alt_text) > 20 or len(title_text) > 20:
                    relevance_score += 0.5

                # Bonus cho ·∫£nh t·ª´ document c√≥ ƒëi·ªÉm cao
                if hasattr(doc, 'search_weight'):
                    relevance_score *= doc.search_weight

                # Th√™m ·∫£nh n·∫øu c√≥ ƒëi·ªÉm ho·∫∑c kh√¥ng c√≥ m√¥ t·∫£ (fallback)
                if relevance_score > 0 or (not alt_text and not title_text):
                    img_path = Path(img_info.get('local_path', ''))
                    if img_path.exists():
                        relevant_images.append({
                            'path': img_path,
                            'alt': img_info.get('alt', ''),
                            'title': img_info.get('title', ''),
                            'url': img_info.get('url', ''),
                            'source_doc': doc,
                            'relevance_score': relevance_score
                        })

        # S·∫Øp x·∫øp theo ƒëi·ªÉm v√† lo·∫°i b·ªè tr√πng l·∫∑p
        seen_paths = set()
        unique_images = []
        for img in sorted(relevant_images, key=lambda x: x['relevance_score'], reverse=True):
            if img['path'] not in seen_paths:
                unique_images.append(img)
                seen_paths.add(img['path'])

        return unique_images[:max_images]

def display_database_debug_info(rag_system):
    """Hi·ªÉn th·ªã th√¥ng tin debug chi ti·∫øt v·ªÅ database"""
    st.subheader("üîç Th√¥ng tin chi ti·∫øt Database")
    
    db_info = rag_system.get_database_info()
    
    # Metrics t·ªïng quan
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Documents", db_info['total_documents'])
    with col2:
        st.metric("T·ªïng t·ª´", f"{db_info['total_words']:,}")
    with col3:
        st.metric("T·ªïng ·∫£nh", db_info['total_images'])
    with col4:
        st.metric("Lo·∫°i n·ªôi dung", len(db_info['content_types']))

    # Chi ti·∫øt content types
    if db_info['content_types']:
        st.write("**Ph√¢n lo·∫°i n·ªôi dung:**")
        for content_type, count in db_info['content_types'].items():
            st.write(f"- {content_type}: {count} documents")

    # Danh s√°ch documents
    with st.expander("üìã Danh s√°ch Documents trong Database", expanded=False):
        for i, title in enumerate(db_info['titles'], 1):
            st.write(f"{i}. **{title}**")
            if i-1 < len(db_info['urls']):
                st.caption(f"URL: {db_info['urls'][i-1]}")

    # Sample content
    with st.expander("üìÑ M·∫´u n·ªôi dung", expanded=False):
        for sample in db_info['sample_content'][:3]:
            st.write(f"**{sample['title']}**")
            st.write(sample['sample'])
            st.write("---")

def get_server_info(config):
    """L·∫•y th√¥ng tin server model API"""
    server_info = {
        'status': 'unknown',
        'base_url': config.get('LOCAL_MODEL', {}).get('base_url', 'N/A'),
        'model': config.get('LOCAL_MODEL', {}).get('model', 'N/A'),
        'connection_status': False,
        'response_time': None
    }

    try:
        base_url = server_info['base_url']
        if not base_url or base_url == 'N/A':
            server_info['status'] = 'No URL configured'
            return server_info

        local_client = LocalGemmaClient(base_url=base_url, model=server_info['model'])
        start_time = time.time()
        connection_status = local_client.check_connection()
        response_time = time.time() - start_time

        server_info['connection_status'] = connection_status
        server_info['response_time'] = response_time

        if connection_status:
            server_info['status'] = 'Connected'
        else:
            server_info['status'] = 'Connection Failed'

    except Exception as e:
        server_info['status'] = f'Error: {str(e)}'
        logger.error(f"L·ªói khi l·∫•y th√¥ng tin server: {e}")

    return server_info

def display_server_info():
    """Hi·ªÉn th·ªã th√¥ng tin server trong sidebar"""
    config = load_config_local()
    server_info = get_server_info(config)

    st.sidebar.header("üñ•Ô∏è Th√¥ng tin Server")

    # Status indicator
    status = server_info['status']
    if server_info['connection_status']:
        st.sidebar.success(f"‚úÖ {status}")
    elif 'Error' in status or 'Failed' in status:
        st.sidebar.error(f"‚ùå {status}")
    else:
        st.sidebar.warning(f"‚ö†Ô∏è {status}")

    # Server details
    with st.sidebar.expander("Chi ti·∫øt Server", expanded=False):
        st.write(f"**URL:** {server_info['base_url']}")
        if server_info['response_time']:
            st.write(f"**Th·ªùi gian ph·∫£n h·ªìi:** {server_info['response_time']:.3f}s")

    # Model info
    st.write(f"**Model hi·ªán t·∫°i:** {server_info['model']}")

def create_sidebar():
    """T·∫°o sidebar v·ªõi c√°c ch·ª©c nƒÉng ƒëi·ªÅu khi·ªÉn"""
    st.sidebar.title("üîß ƒêi·ªÅu khi·ªÉn h·ªá th·ªëng")
    display_server_info()

    # Ph·∫ßn nh·∫≠p URL m·ªõi
    st.sidebar.header("üì• Thu th·∫≠p d·ªØ li·ªáu m·ªõi")
    with st.sidebar.expander("Th√™m URL m·ªõi", expanded=False):
        new_url = st.text_input(
            "Nh·∫≠p URL c·∫ßn thu th·∫≠p:",
            placeholder="https://example.com/article",
            key="new_url_input"
        )

        if st.button("Thu th·∫≠p", key="collect_single", type="primary"):
            if new_url:
                collect_single_url(new_url)
            else:
                st.error("Vui l√≤ng nh·∫≠p URL")

def collect_single_url(url):
    """Thu th·∫≠p d·ªØ li·ªáu t·ª´ m·ªôt URL"""
    try:
        if not url.startswith(('http://', 'https://')):
            st.error("URL ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng http:// ho·∫∑c https://")
            return

        with st.spinner(f"ƒêang thu th·∫≠p t·ª´ {url}..."):
            scraper = WebScraperLocal(overwrite=False)
            success = scraper.fetch_and_save(url)
            
            if success:
                st.success(f"‚úÖ Thu th·∫≠p th√†nh c√¥ng t·ª´ {url}")
                if 'rag_system' in st.session_state:
                    st.session_state.rag_system.load_multimodal_data()
                st.rerun()
            else:
                st.error(f"‚ùå Kh√¥ng th·ªÉ thu th·∫≠p t·ª´ {url}")
    except Exception as e:
        st.error(f"L·ªói: {e}")
        logger.error(f"L·ªói thu th·∫≠p {url}: {e}")

def handle_no_results_fallback(question, config):
    """X·ª≠ l√Ω fallback khi kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ trong database"""
    st.info("üîç Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong database. ƒêang g·ªçi AI ƒë·ªÉ tr·∫£ l·ªùi...")
    
    fallback_context = f"""
    C√¢u h·ªèi: {question}
    
    Th√¥ng tin: Kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ trong c∆° s·ªü d·ªØ li·ªáu v·ªÅ c√¢u h·ªèi n√†y.
    
    H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
    1. Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung v·ªÅ ch·ªß ƒë·ªÅ ƒë∆∞·ª£c h·ªèi
    2. N√™u r√µ r·∫±ng ƒë√¢y l√† c√¢u tr·∫£ l·ªùi chung, kh√¥ng d·ª±a tr√™n d·ªØ li·ªáu c·ª• th·ªÉ
    3. ƒê·ªÅ xu·∫•t ng∆∞·ªùi d√πng t√¨m ki·∫øm th√™m th√¥ng tin ho·∫∑c cung c·∫•p th√™m d·ªØ li·ªáu
    4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch chi ti·∫øt v√† h·ªØu √≠ch
    """
    
    try:
        response = ask_local_model(question, fallback_context, config)
        st.warning("‚ö†Ô∏è C√¢u tr·∫£ l·ªùi d∆∞·ªõi ƒë√¢y d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa AI, kh√¥ng d·ª±a tr√™n d·ªØ li·ªáu trong database c·ªßa b·∫°n.")
        return response
    except Exception as e:
        logger.error(f"Error in fallback AI response: {e}")
        return f"Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y v√¨ kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong database v√† kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi AI."

def create_intelligent_multimodal_context(text_context, relevant_images, question):
    """T·∫°o context ƒëa ph∆∞∆°ng ti·ªán th√¥ng minh v·ªõi h∆∞·ªõng d·∫´n ch√®n ·∫£nh"""
    context = f"Th√¥ng tin vƒÉn b·∫£n:\n{text_context}\n\n"
    
    if relevant_images:
        context += "Th√¥ng tin h√¨nh ·∫£nh c√≥ s·∫µn:\n"
        for i, img_info in enumerate(relevant_images, 1):
            context += f"\n[IMAGE_{i}]:"
            if img_info['alt']:
                context += f" M√¥ t·∫£: {img_info['alt']}"
            if img_info['title']:
                context += f" Ti√™u ƒë·ªÅ: {img_info['title']}"
            context += f" (Ngu·ªìn: {img_info['source_doc']['title']})"
            if img_info['relevance_score'] > 0:
                context += f" (ƒê·ªô li√™n quan: {img_info['relevance_score']:.1f})"

    context += f"""

QUAN TR·ªåNG: Khi tr·∫£ l·ªùi c√¢u h·ªèi "{question}", h√£y:
1. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt v√† c√≥ c·∫•u tr√∫c
2. Khi ƒë·ªÅ c·∫≠p ƒë·∫øn h√¨nh ·∫£nh, s·ª≠ d·ª•ng c√∫ ph√°p [IMAGE_X] ƒë·ªÉ ch·ªâ ƒë·ªãnh ·∫£nh n√†o c·∫ßn hi·ªÉn th·ªã
3. V√≠ d·ª•: "Nh∆∞ b·∫°n c√≥ th·ªÉ th·∫•y trong [IMAGE_1], ƒëi·ªÅu n√†y cho th·∫•y..."
4. S·ª≠ d·ª•ng [IMAGE_X] ·ªü nh·ªØng v·ªã tr√≠ ph√π h·ª£p trong c√¢u tr·∫£ l·ªùi ƒë·ªÉ minh h·ªça n·ªôi dung
5. M·ªói [IMAGE_X] s·∫Ω ƒë∆∞·ª£c thay th·∫ø b·∫±ng h√¨nh ·∫£nh t∆∞∆°ng ·ª©ng khi hi·ªÉn th·ªã
6. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát v√† d·ª±a tr√™n th√¥ng tin ƒë√£ cung c·∫•p

H√£y tham kh·∫£o c·∫£ th√¥ng tin vƒÉn b·∫£n v√† h√¨nh ·∫£nh ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán v√† c√≥ minh h·ªça ph√π h·ª£p."""
    
    return context

def parse_answer_with_image_markers(answer, relevant_images):
    """Ph√¢n t√≠ch c√¢u tr·∫£ l·ªùi v√† t√°ch c√°c marker ·∫£nh"""
    image_pattern = r'\[IMAGE_(\d+)\]'
    parts = re.split(image_pattern, answer)
    
    content_parts = []
    for i, part in enumerate(parts):
        if part.isdigit():
            image_index = int(part) - 1
            if 0 <= image_index < len(relevant_images):
                content_parts.append({
                    'type': 'image',
                    'content': relevant_images[image_index],
                    'index': image_index
                })
        else:
            if part.strip():
                content_parts.append({
                    'type': 'text',
                    'content': part.strip()
                })
    
    return content_parts

def smart_display_answer_with_embedded_images(answer, relevant_images):
    """Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi v·ªõi ·∫£nh ƒë∆∞·ª£c ch√®n th√¥ng minh"""
    if not relevant_images:
        st.markdown(answer)
        return

    content_parts = parse_answer_with_image_markers(answer, relevant_images)
    
    if not any(part['type'] == 'image' for part in content_parts):
        # N·∫øu AI kh√¥ng s·ª≠ d·ª•ng [IMAGE_X], t·ª± ƒë·ªông ch√®n ·∫£nh
        auto_embed_images_in_answer(answer, relevant_images)
        return

    # Hi·ªÉn th·ªã theo th·ª© t·ª± AI ƒë√£ ch·ªâ ƒë·ªãnh
    for part in content_parts:
        if part['type'] == 'text':
            st.markdown(part['content'])
        elif part['type'] == 'image':
            img_info = part['content']
            try:
                image = Image.open(img_info['path'])
                caption = ""
                if img_info.get('alt'):
                    caption = f"üì∑ {img_info['alt']}"
                elif img_info.get('title'):
                    caption = f"üì∑ {img_info['title']}"
                else:
                    caption = f"üì∑ H√¨nh ·∫£nh t·ª´ {img_info['source_doc']['title']}"

                # Hi·ªÉn th·ªã ·∫£nh v·ªõi caption
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    st.image(image, caption=caption, use_container_width=True)
                st.markdown("", unsafe_allow_html=True)  # Spacing
            except Exception as e:
                st.error(f"Kh√¥ng th·ªÉ hi·ªÉn th·ªã ·∫£nh: {e}")

def auto_embed_images_in_answer(answer, relevant_images):
    """T·ª± ƒë·ªông ch√®n ·∫£nh v√†o c√¢u tr·∫£ l·ªùi n·∫øu AI kh√¥ng s·ª≠ d·ª•ng [IMAGE_X]"""
    paragraphs = [p.strip() for p in answer.split('\n\n') if p.strip()]
    
    if len(paragraphs) <= 1:
        # N·∫øu kh√¥ng c√≥ ƒëo·∫°n vƒÉn, chia theo c√¢u
        sentences = [s.strip() + '.' for s in answer.split('.') if s.strip()]
        paragraphs = sentences

    total_parts = len(paragraphs)
    total_images = len(relevant_images)

    if total_images == 0:
        st.markdown(answer)
        return

    # T√≠nh to√°n v·ªã tr√≠ ch√®n ·∫£nh
    insert_positions = []
    if total_parts > 1:
        step = max(1, total_parts // (total_images + 1))
        for i in range(min(total_images, total_parts - 1)):
            pos = (i + 1) * step
            if pos < total_parts:
                insert_positions.append(pos)

    # Hi·ªÉn th·ªã v·ªõi ·∫£nh ƒë∆∞·ª£c ch√®n
    image_index = 0
    for i, paragraph in enumerate(paragraphs):
        st.markdown(paragraph)
        
        # Ch√®n ·∫£nh t·∫°i v·ªã tr√≠ ƒë∆∞·ª£c t√≠nh to√°n
        if i in insert_positions and image_index < len(relevant_images):
            img_info = relevant_images[image_index]
            try:
                image = Image.open(img_info['path'])
                caption = ""
                if img_info.get('alt'):
                    caption = f"üì∑ {img_info['alt']}"
                elif img_info.get('title'):
                    caption = f"üì∑ {img_info['title']}"
                else:
                    caption = f"üì∑ H√¨nh ·∫£nh minh h·ªça"

                # Spacing tr∆∞·ªõc ·∫£nh
                st.markdown("", unsafe_allow_html=True)
                
                # Hi·ªÉn th·ªã ·∫£nh ·ªü gi·ªØa
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    st.image(image, caption=caption, use_container_width=True)
                
                # Spacing sau ·∫£nh
                st.markdown("", unsafe_allow_html=True)
                image_index += 1
            except Exception as e:
                st.error(f"Kh√¥ng th·ªÉ hi·ªÉn th·ªã ·∫£nh: {e}")

def main():
    """H√†m ch√≠nh c·ªßa ·ª©ng d·ª•ng v·ªõi ·∫£nh ch√®n trong n·ªôi dung"""
    st.set_page_config(
        page_title="RAG Local v·ªõi ·∫¢nh Ch√®n",
        page_icon="ü§ñ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # T·∫°o sidebar
    create_sidebar()

    # Header ch√≠nh
    st.title("ü§ñ H·ªá th·ªëng RAG Local v·ªõi ·∫¢nh Ch√®n Th√¥ng Minh")
    st.markdown("---")

    # Kh·ªüi t·∫°o RAG system
    if 'rag_system' not in st.session_state:
        with st.spinner("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng..."):
            st.session_state.rag_system = EnhancedMultimodalRAGLocal("db")

    rag_system = st.session_state.rag_system

    # Ki·ªÉm tra d·ªØ li·ªáu
    if not rag_system.has_data:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng thu th·∫≠p d·ªØ li·ªáu t·ª´ sidebar.")
        
        col1, col2 = st.columns(2)
        with col1:
            st.info("üí° **H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:**")
            st.write("1. S·ª≠ d·ª•ng sidebar b√™n tr√°i ƒë·ªÉ th√™m URL")
            st.write("2. Nh·∫•n 'Thu th·∫≠p' ƒë·ªÉ t·∫£i d·ªØ li·ªáu")
            st.write("3. Sau khi c√≥ d·ªØ li·ªáu, b·∫°n c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi")
        
        with col2:
            st.info("üîß **T√≠nh nƒÉng c√≥ s·∫µn:**")
            st.write("- Thu th·∫≠p t·ª´ URL ƒë∆°n l·∫ª")
            st.write("- ·∫¢nh ch√®n th√¥ng minh trong c√¢u tr·∫£ l·ªùi")
            st.write("- AI fallback khi kh√¥ng t√¨m th·∫•y")
            st.write("- Debug th√¥ng tin database")
        
        return

    # Hi·ªÉn th·ªã th√¥ng tin database
    display_database_debug_info(rag_system)
    st.markdown("---")

    # Ph·∫ßn h·ªèi ƒë√°p ch√≠nh
    st.header("üí¨ H·ªèi ƒë√°p th√¥ng minh v·ªõi ·∫¢nh Minh H·ªça")

    # Input c√¢u h·ªèi
    question = st.text_input(
        "ƒê·∫∑t c√¢u h·ªèi:",
        placeholder="V√≠ d·ª•: c√°ch c√†i ƒë·∫∑t Oracle 19c, ZooKeeper l√† g√¨...",
        help="Nh·∫≠p c√¢u h·ªèi v√† nh·∫•n Enter ƒë·ªÉ t√¨m ki·∫øm"
    )

    # T√πy ch·ªçn n√¢ng cao
    with st.expander("‚öôÔ∏è T√πy ch·ªçn t√¨m ki·∫øm", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            threshold = st.slider(
                "Ng∆∞·ª°ng t√¨m ki·∫øm:",
                min_value=0.01,
                max_value=0.8,
                value=0.05,
                step=0.01,
                help="Ng∆∞·ª°ng th·∫•p h∆°n = k·∫øt qu·∫£ nhi·ªÅu h∆°n nh∆∞ng √≠t ch√≠nh x√°c h∆°n"
            )
        
        with col2:
            max_results = st.selectbox(
                "S·ªë k·∫øt qu·∫£ t·ªëi ƒëa:",
                options=[1, 2, 3, 4, 5],
                index=2
            )

    if question:
        with st.spinner("üîç ƒêang t√¨m ki·∫øm v√† t·∫°o c√¢u tr·∫£ l·ªùi..."):
            try:
                # Hi·ªÉn th·ªã debug info
                st.info(f"üîç ƒêang t√¨m ki·∫øm: '{question}' v·ªõi threshold {threshold}")
                
                # T√¨m ki·∫øm documents li√™n quan
                relevant_docs, similarity = rag_system.enhanced_search_multimodal(
                    question, threshold, max_results
                )

                # Debug th√¥ng tin t√¨m ki·∫øm chi ti·∫øt
                st.subheader("üîç K·∫øt qu·∫£ t√¨m ki·∫øm")
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ƒê·ªô t∆∞∆°ng t·ª± t·ªëi ƒëa", f"{similarity:.3f}")
                with col2:
                    st.metric("S·ªë documents t√¨m th·∫•y", len(relevant_docs))
                with col3:
                    st.metric("Ng∆∞·ª°ng s·ª≠ d·ª•ng", f"{threshold:.3f}")
                with col4:
                    # Hi·ªÉn th·ªã threshold ƒë·ªÅ xu·∫•t
                    suggested_threshold = max(0.01, similarity * 0.8) if similarity > 0 else 0.01
                    st.metric("Threshold ƒë·ªÅ xu·∫•t", f"{suggested_threshold:.3f}")

                # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ c√°c ph∆∞∆°ng ph√°p kh√°c
                if not relevant_docs:
                    st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y v·ªõi t√¨m ki·∫øm ch√≠nh, ƒëang th·ª≠ c√°c ph∆∞∆°ng ph√°p kh√°c...")
                    
                    # Th·ª≠ t√¨m ki·∫øm fuzzy
                    words = question.split()
                    st.write("**ƒêang th·ª≠ t√¨m ki·∫øm t·ª´ng t·ª´:**")
                    for word in words:
                        if len(word) > 2:
                            word_results, word_sim = rag_system.enhanced_search_multimodal(word, 0.01, 1)
                            st.write(f"- '{word}': {len(word_results)} k·∫øt qu·∫£ (sim: {word_sim:.3f})")
                            if word_results:
                                relevant_docs = word_results
                                similarity = word_sim
                                st.success(f"‚úÖ T√¨m th·∫•y k·∫øt qu·∫£ v·ªõi t·ª´ kh√≥a: '{word}'")
                                break

                if relevant_docs:
                    # Hi·ªÉn th·ªã documents t√¨m th·∫•y
                    with st.expander("üìã Documents ƒë∆∞·ª£c t√¨m th·∫•y", expanded=True):
                        for i, doc in enumerate(relevant_docs, 1):
                            st.write(f"**{i}. {doc['title']}**")
                            st.caption(f"URL: {doc['url']}")
                            st.caption(f"Lo·∫°i: {doc['metadata'].get('content_type', 'unknown')}")
                            st.caption(f"S·ªë t·ª´: {doc['metadata'].get('word_count', 0)}")
                            
                            # Show snippet v·ªõi highlight
                            snippet = doc['text_content'][:300] + "..."
                            # Highlight query words trong snippet
                            highlighted_snippet = snippet
                            for word in question.split():
                                if len(word) > 3:
                                    highlighted_snippet = highlighted_snippet.replace(
                                        word, f"**{word}**"
                                    )
                            st.write(f"*ƒêo·∫°n tr√≠ch:* {highlighted_snippet}")
                            st.write("---")

                    # T·∫°o context v√† tr·∫£ l·ªùi
                    text_context = "\n\n".join([
                        f"Ti√™u ƒë·ªÅ: {doc['title']}\nM√¥ t·∫£: {doc['description']}\nN·ªôi dung: {doc['text_content'][:1000]}..."
                        for doc in relevant_docs
                    ])

                    relevant_images = rag_system.get_relevant_images_for_context(
                        relevant_docs, question, 5
                    )

                    context = create_intelligent_multimodal_context(
                        text_context, relevant_images, question
                    )

                    config = load_config_local()
                    answer = ask_local_model(question, context, config)

                    # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi ·∫£nh ch√®n th√¥ng minh
                    st.subheader("üìù C√¢u tr·∫£ l·ªùi:")
                    smart_display_answer_with_embedded_images(answer, relevant_images)

                    # Hi·ªÉn th·ªã th√¥ng tin ·∫£nh ƒë√£ s·ª≠ d·ª•ng
                    if relevant_images:
                        with st.expander("üñºÔ∏è Th√¥ng tin ·∫£nh ƒë√£ s·ª≠ d·ª•ng", expanded=False):
                            for i, img_info in enumerate(relevant_images, 1):
                                st.write(f"**·∫¢nh {i}:**")
                                st.write(f"- Ngu·ªìn: {img_info['source_doc']['title']}")
                                if img_info['alt']:
                                    st.write(f"- M√¥ t·∫£: {img_info['alt']}")
                                if img_info['title']:
                                    st.write(f"- Ti√™u ƒë·ªÅ: {img_info['title']}")
                                st.write(f"- ƒê·ªô li√™n quan: {img_info['relevance_score']:.2f}")
                                st.write("---")

                else:
                    # Fallback mechanism - g·ªçi AI tr·ª±c ti·∫øp
                    st.subheader("üìù C√¢u tr·∫£ l·ªùi (AI Fallback):")
                    config = load_config_local()
                    fallback_answer = handle_no_results_fallback(question, config)
                    st.markdown(fallback_answer)
                    
                    st.info("üí° **G·ª£i √Ω c·∫£i thi·ªán:**")
                    st.write("- Database c√≥ th·ªÉ ch∆∞a ch·ª©a th√¥ng tin v·ªÅ ch·ªß ƒë·ªÅ n√†y")
                    st.write("- Th·ª≠ s·ª≠ d·ª•ng t·ª´ kh√≥a ƒë∆°n gi·∫£n h∆°n")
                    st.write("- Thu th·∫≠p th√™m d·ªØ li·ªáu li√™n quan t·ª´ sidebar")

            except Exception as e:
                st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")
                logger.error(f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {e}")
                
                # Emergency fallback
                st.info("üîÑ ƒêang th·ª≠ ph∆∞∆°ng ph√°p d·ª± ph√≤ng...")
                try:
                    config = load_config_local()
                    emergency_answer = handle_no_results_fallback(question, config)
                    st.subheader("üìù C√¢u tr·∫£ l·ªùi (D·ª± ph√≤ng):")
                    st.markdown(emergency_answer)
                except Exception as e2:
                    st.error(f"‚ùå Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi: {e2}")
                    st.info("Vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c ki·ªÉm tra k·∫øt n·ªëi v·ªõi model local.")

    # Footer
    st.markdown("---")
    st.markdown(
        "ü§ñ **RAG Local System v·ªõi ·∫¢nh Ch√®n Th√¥ng Minh** - "
        "Powered by Enhanced Search & Streamlit"
    )

if __name__ == "__main__":
    main()

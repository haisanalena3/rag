import os
import json
import streamlit as st
from pathlib import Path
from PIL import Image
import base64
from io import BytesIO
import traceback
import numpy as np
import re
import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import hashlib
import time
from datetime import datetime
import shutil

from config_local import load_config_local
from rag_local import (
    load_documents, search_documents, ask_local_model,
    search_documents_with_threshold, adaptive_threshold_search
)
from local_gemma import LocalGemmaClient
from collect_local import WebScraperLocal

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedMultimodalRAGLocal:
    def __init__(self, db_dir):
        self.db_dir = Path(db_dir)
        self.documents = []
        self.has_data = False
        self.query_history = []
        self.load_multimodal_data()

    def load_multimodal_data(self):
        """Load d·ªØ li·ªáu ƒëa ph∆∞∆°ng ti·ªán t·ª´ database"""
        self.documents = []
        logger.info(f"Ki·ªÉm tra th∆∞ m·ª•c db: {self.db_dir.absolute()}")
        
        if not self.db_dir.exists():
            logger.warning(f"Th∆∞ m·ª•c db kh√¥ng t·ªìn t·∫°i: {self.db_dir}")
            self.has_data = False
            return

        site_dirs = [d for d in self.db_dir.iterdir() if d.is_dir()]
        logger.info(f"T√¨m th·∫•y {len(site_dirs)} th∆∞ m·ª•c con trong db")

        for site_dir in site_dirs:
            logger.info(f"Ki·ªÉm tra th∆∞ m·ª•c: {site_dir.name}")
            metadata_file = site_dir / "metadata.json"
            
            if not metadata_file.exists():
                logger.warning(f"Thi·∫øu metadata.json trong {site_dir.name}")
                continue

            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                content_file = site_dir / "content.txt"
                text_content = ""
                if content_file.exists():
                    with open(content_file, 'r', encoding='utf-8') as f:
                        text_content = f.read()

                doc = {
                    'site_name': site_dir.name,
                    'url': data['metadata']['url'],
                    'title': data['metadata']['title'],
                    'description': data['metadata']['description'],
                    'text_content': text_content,
                    'images': data.get('images', []),
                    'image_dir': site_dir / "images",
                    'metadata': data['metadata']
                }

                self.documents.append(doc)
                logger.info(f"Th√™m document: {doc['title']}")

            except Exception as e:
                logger.error(f"L·ªói load d·ªØ li·ªáu t·ª´ {site_dir}: {e}")

        self.has_data = len(self.documents) > 0
        logger.info(f"T·ªïng c·ªông: {len(self.documents)} documents")

    def analyze_query_intent(self, query):
        """Ph√¢n t√≠ch √Ω ƒë·ªãnh c·ªßa c√¢u h·ªèi"""
        query_lower = query.lower()
        
        intent_keywords = {
            'visual': ['·∫£nh', 'h√¨nh', 'm√†u', 'nh√¨n', 'th·∫•y', 'hi·ªÉn th·ªã', 'minh h·ªça', 'h√¨nh ·∫£nh'],
            'descriptive': ['m√¥ t·∫£', 'gi·∫£i th√≠ch', 'l√† g√¨', 'nh∆∞ th·∫ø n√†o', 't·∫°i sao', 'ƒë·ªãnh nghƒ©a'],
            'comparative': ['so s√°nh', 'kh√°c nhau', 'gi·ªëng', 't∆∞∆°ng t·ª±', 'h∆°n', 'kh√°c bi·ªát'],
            'instructional': ['c√°ch', 'l√†m', 'th·ª±c hi·ªán', 'b∆∞·ªõc', 'h∆∞·ªõng d·∫´n', 'ph∆∞∆°ng ph√°p'],
            'factual': ['khi n√†o', '·ªü ƒë√¢u', 'ai', 'bao nhi√™u', 's·ªë l∆∞·ª£ng', 'th·ªëng k√™']
        }

        intent_scores = {}
        for intent, keywords in intent_keywords.items():
            score = sum(1 for keyword in keywords if keyword in query_lower)
            intent_scores[intent] = score

        primary_intent = max(intent_scores, key=intent_scores.get) if intent_scores else 'general'
        return primary_intent, intent_scores

    def enhanced_search_multimodal(self, query, threshold=0.2, top_k=3):
        """T√¨m ki·∫øm ƒëa ph∆∞∆°ng ti·ªán n√¢ng cao"""
        if not self.has_data:
            return [], 0

        primary_intent, intent_scores = self.analyze_query_intent(query)

        text_corpus = []
        doc_weights = []

        for doc in self.documents:
            combined_text = f"{doc['title']}\n{doc['description']}\n{doc['text_content']}"
            
            image_descriptions = []
            for img in doc['images']:
                if img.get('alt'):
                    image_descriptions.append(img['alt'])
                if img.get('title'):
                    image_descriptions.append(img['title'])

            # ƒêi·ªÅu ch·ªânh tr·ªçng s·ªë d·ª±a tr√™n intent
            if primary_intent == 'visual' and image_descriptions:
                combined_text += "\n" + "\n".join(image_descriptions) * 2
                doc_weights.append(1.5)
            elif primary_intent == 'descriptive':
                combined_text = doc['description'] + "\n" + combined_text
                doc_weights.append(1.2)
            else:
                combined_text += "\n" + "\n".join(image_descriptions)
                doc_weights.append(1.0)

            text_corpus.append(combined_text)

        try:
            relevant_texts, max_similarity = adaptive_threshold_search(
                query, text_corpus, threshold
            )

            relevant_docs = []
            for relevant_text in relevant_texts:
                for i, text in enumerate(text_corpus):
                    if text == relevant_text:
                        doc = self.documents[i].copy()
                        doc['search_weight'] = doc_weights[i]
                        relevant_docs.append(doc)
                        break

            relevant_docs.sort(key=lambda x: x.get('search_weight', 1.0), reverse=True)

            # L∆∞u l·ªãch s·ª≠
            self.query_history.append({
                'query': query,
                'intent': primary_intent,
                'similarity': max_similarity,
                'results_count': len(relevant_docs)
            })

            return relevant_docs[:top_k], max_similarity

        except Exception as e:
            logger.error(f"L·ªói enhanced search: {e}")
            return [], 0

    def get_relevant_images_for_context(self, relevant_docs, query, max_images=5):
        """L·∫•y ·∫£nh li√™n quan ƒë·ªÉ ƒë∆∞a v√†o context"""
        relevant_images = []
        
        for doc in relevant_docs:
            for img_info in doc['images']:
                relevance_score = 0
                alt_text = img_info.get('alt', '').lower()
                title_text = img_info.get('title', '').lower()
                query_lower = query.lower()

                # T√≠nh ƒëi·ªÉm d·ª±a tr√™n t·ª´ kh√≥a
                for word in query_lower.split():
                    if word in alt_text or word in title_text:
                        relevance_score += 1

                # Bonus ƒëi·ªÉm cho ·∫£nh c√≥ m√¥ t·∫£ chi ti·∫øt
                if len(alt_text) > 20 or len(title_text) > 20:
                    relevance_score += 0.5

                if relevance_score > 0 or (not alt_text and not title_text):
                    img_path = Path(img_info.get('local_path', ''))
                    if img_path.exists():
                        relevant_images.append({
                            'path': img_path,
                            'alt': img_info.get('alt', ''),
                            'title': img_info.get('title', ''),
                            'url': img_info.get('url', ''),
                            'source_doc': doc,
                            'relevance_score': relevance_score
                        })

        relevant_images.sort(key=lambda x: x['relevance_score'], reverse=True)
        return relevant_images[:max_images]

    def get_search_statistics(self):
        """Th·ªëng k√™ hi·ªáu su·∫•t t√¨m ki·∫øm"""
        if not self.query_history:
            return {}

        total_queries = len(self.query_history)
        successful_queries = sum(1 for q in self.query_history if q['results_count'] > 0)
        avg_similarity = np.mean([q['similarity'] for q in self.query_history])

        intent_distribution = {}
        for q in self.query_history:
            intent = q['intent']
            intent_distribution[intent] = intent_distribution.get(intent, 0) + 1

        return {
            'total_queries': total_queries,
            'success_rate': successful_queries / total_queries * 100,
            'average_similarity': avg_similarity,
            'intent_distribution': intent_distribution
        }

    def get_database_statistics(self):
        """Th·ªëng k√™ database"""
        if not self.has_data:
            return {}
        
        total_images = 0
        total_text_length = 0
        sites_info = []
        
        for doc in self.documents:
            total_images += len(doc['images'])
            total_text_length += len(doc['text_content'])
            
            # T√≠nh k√≠ch th∆∞·ªõc th∆∞ m·ª•c
            site_dir = self.db_dir / doc['site_name']
            site_size = 0
            if site_dir.exists():
                for file_path in site_dir.rglob('*'):
                    if file_path.is_file():
                        site_size += file_path.stat().st_size
            
            sites_info.append({
                'name': doc['title'],
                'url': doc['url'],
                'images': len(doc['images']),
                'text_length': len(doc['text_content']),
                'size_bytes': site_size,
                'scraped_at': doc['metadata'].get('scraped_at', 'N/A')
            })
        
        return {
            'total_documents': len(self.documents),
            'total_images': total_images,
            'total_text_length': total_text_length,
            'sites_info': sites_info
        }

def check_ollama_connection(base_url):
    """Ki·ªÉm tra k·∫øt n·ªëi t·ªõi Ollama server"""
    try:
        # ƒê·∫£m b·∫£o URL ƒë√∫ng format
        if isinstance(base_url, dict):
            logger.error(f"base_url kh√¥ng ƒë√∫ng format: {base_url}")
            return False, "URL kh√¥ng ƒë√∫ng format", "base_url ph·∫£i l√† string"
        
        clean_url = str(base_url).rstrip('/')
        if not clean_url.startswith(('http://', 'https://')):
            clean_url = f"https://{clean_url}"
        
        logger.info(f"Ki·ªÉm tra k·∫øt n·ªëi t·ªõi: {clean_url}")
        
        # Th·ª≠ endpoint health check
        response = requests.get(clean_url, timeout=5)
        logger.info(f"Response status: {response.status_code}")
        
        if response.status_code == 200:
            return True, "K·∫øt n·ªëi th√†nh c√¥ng", response.text.strip()
        else:
            return False, f"Server ph·∫£n h·ªìi kh√¥ng ƒë√∫ng: {response.status_code}", response.text
            
    except requests.exceptions.ConnectionError:
        return False, "Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi server", "Connection refused"
    except requests.exceptions.Timeout:
        return False, "Timeout khi k·∫øt n·ªëi", "Request timeout"
    except Exception as e:
        logger.error(f"L·ªói ki·ªÉm tra k·∫øt n·ªëi: {e}")
        return False, f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {str(e)}", str(e)

def get_ollama_models(base_url):
    """L·∫•y danh s√°ch models t·ª´ Ollama"""
    try:
        clean_url = str(base_url).rstrip('/')
        if not clean_url.startswith(('http://', 'https://')):
            clean_url = f"https://{clean_url}"
            
        models_url = f"{clean_url}/api/tags"
        logger.info(f"G·ªçi API tags: {models_url}")
        
        response = requests.get(models_url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            return True, data.get('models', [])
        else:
            logger.warning(f"API tags tr·∫£ v·ªÅ status: {response.status_code}")
            return False, []
            
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y danh s√°ch models: {e}")
        return False, []

def get_running_models(base_url):
    """L·∫•y danh s√°ch models ƒëang ch·∫°y"""
    try:
        clean_url = str(base_url).rstrip('/')
        if not clean_url.startswith(('http://', 'https://')):
            clean_url = f"https://{clean_url}"
            
        ps_url = f"{clean_url}/api/ps"
        response = requests.get(ps_url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            models = data.get('models', [])
            
            # L√†m s·∫°ch d·ªØ li·ªáu models
            cleaned_models = []
            for model in models:
                if isinstance(model, dict):
                    cleaned_model = {
                        'name': str(model.get('name', 'Unknown')),
                        'size': model.get('size'),
                        'expires_at': model.get('expires_at')
                    }
                    cleaned_models.append(cleaned_model)
            
            return True, cleaned_models
        else:
            return False, []
            
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y running models: {e}")
        return False, []

def get_model_info(base_url, model_name):
    """L·∫•y th√¥ng tin chi ti·∫øt c·ªßa model"""
    try:
        clean_url = str(base_url).rstrip('/')
        if not clean_url.startswith(('http://', 'https://')):
            clean_url = f"https://{clean_url}"
            
        show_url = f"{clean_url}/api/show"
        payload = {"name": model_name}
        response = requests.post(show_url, json=payload, timeout=15)
        
        if response.status_code == 200:
            return True, response.json()
        else:
            return False, {}
            
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y th√¥ng tin model {model_name}: {e}")
        return False, {}

def format_model_size(size_bytes):
    """Format k√≠ch th∆∞·ªõc model"""
    if size_bytes is None:
        return "N/A"
    
    try:
        if isinstance(size_bytes, str):
            size_bytes = int(size_bytes)
        elif not isinstance(size_bytes, (int, float)):
            return "N/A"
    except (ValueError, TypeError):
        return "N/A"
    
    if size_bytes <= 0:
        return "0 B"
    
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} PB"

def format_duration(nanoseconds):
    """Format th·ªùi gian t·ª´ nanoseconds"""
    if nanoseconds is None:
        return "N/A"
    
    try:
        if isinstance(nanoseconds, str):
            nanoseconds = int(nanoseconds)
        elif not isinstance(nanoseconds, (int, float)):
            return "N/A"
    except (ValueError, TypeError):
        return "N/A"
    
    if nanoseconds <= 0:
        return "0s"
    
    seconds = nanoseconds / 1_000_000_000
    
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f}m"
    else:
        hours = seconds / 3600
        return f"{hours:.1f}h"

def should_use_general_knowledge(relevant_docs, similarity, question):
    """Ki·ªÉm tra xem c√≥ n√™n s·ª≠ d·ª•ng ki·∫øn th·ª©c chung kh√¥ng"""
    MIN_SIMILARITY = 0.2
    
    if not relevant_docs:
        return True
    
    if similarity < MIN_SIMILARITY:
        return True
    
    total_content_length = 0
    for doc in relevant_docs:
        content = doc.get('text_content', '') + doc.get('description', '')
        total_content_length += len(content.strip())
    
    if total_content_length < 100:
        return True
    
    question_words = set(question.lower().split())
    content_words = set()
    
    for doc in relevant_docs:
        content = doc.get('text_content', '') + doc.get('description', '') + doc.get('title', '')
        content_words.update(content.lower().split())
    
    matching_words = question_words.intersection(content_words)
    match_ratio = len(matching_words) / len(question_words) if question_words else 0
    
    if match_ratio < 0.1:
        return True
    
    return False

def create_intelligent_multimodal_context(text_context, relevant_images, question):
    """T·∫°o context ƒëa ph∆∞∆°ng ti·ªán th√¥ng minh"""
    context = f"Th√¥ng tin vƒÉn b·∫£n:\n{text_context}\n\n"
    
    if relevant_images:
        context += "Th√¥ng tin h√¨nh ·∫£nh c√≥ s·∫µn:\n"
        for i, img_info in enumerate(relevant_images, 1):
            context += f"\n[IMAGE_{i}]:"
            if img_info['alt']:
                context += f" M√¥ t·∫£: {img_info['alt']}"
            if img_info['title']:
                context += f" Ti√™u ƒë·ªÅ: {img_info['title']}"
            context += f" (Ngu·ªìn: {img_info['source_doc']['title']})"

    context += f"""

QUAN TR·ªåNG: Khi tr·∫£ l·ªùi c√¢u h·ªèi "{question}", h√£y:
1. ƒê∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt v√† c√≥ c·∫•u tr√∫c
2. Khi ƒë·ªÅ c·∫≠p ƒë·∫øn h√¨nh ·∫£nh, s·ª≠ d·ª•ng c√∫ ph√°p [IMAGE_X] ƒë·ªÉ ch·ªâ ƒë·ªãnh ·∫£nh n√†o c·∫ßn hi·ªÉn th·ªã
3. V√≠ d·ª•: "Nh∆∞ b·∫°n c√≥ th·ªÉ th·∫•y trong [IMAGE_1], ƒëi·ªÅu n√†y cho th·∫•y..."
4. S·ª≠ d·ª•ng [IMAGE_X] ·ªü nh·ªØng v·ªã tr√≠ ph√π h·ª£p trong c√¢u tr·∫£ l·ªùi ƒë·ªÉ minh h·ªça n·ªôi dung
5. M·ªói [IMAGE_X] s·∫Ω ƒë∆∞·ª£c thay th·∫ø b·∫±ng h√¨nh ·∫£nh t∆∞∆°ng ·ª©ng khi hi·ªÉn th·ªã

H√£y tham kh·∫£o c·∫£ th√¥ng tin vƒÉn b·∫£n v√† h√¨nh ·∫£nh ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi to√†n di·ªán v√† c√≥ minh h·ªça ph√π h·ª£p."""

    return context

def create_general_knowledge_context(question):
    """T·∫°o context cho ki·∫øn th·ª©c chung"""
    return f"""{question}"""

def parse_answer_with_image_markers(answer, relevant_images):
    """Ph√¢n t√≠ch c√¢u tr·∫£ l·ªùi v√† t√°ch c√°c marker ·∫£nh"""
    image_pattern = r'\[IMAGE_(\d+)\]'
    parts = re.split(image_pattern, answer)
    
    content_parts = []
    for i, part in enumerate(parts):
        if part.isdigit():
            image_index = int(part) - 1
            if 0 <= image_index < len(relevant_images):
                content_parts.append({
                    'type': 'image',
                    'content': relevant_images[image_index],
                    'index': image_index
                })
        else:
            if part.strip():
                content_parts.append({
                    'type': 'text',
                    'content': part.strip()
                })
    
    return content_parts

def smart_display_answer_with_embedded_images(answer, relevant_images):
    """Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi v·ªõi ·∫£nh ƒë∆∞·ª£c ch√®n th√¥ng minh"""
    if not relevant_images:
        st.markdown(answer)
        return

    content_parts = parse_answer_with_image_markers(answer, relevant_images)
    
    if not any(part['type'] == 'image' for part in content_parts):
        auto_embed_images_in_answer(answer, relevant_images)
        return

    for part in content_parts:
        if part['type'] == 'text':
            st.markdown(part['content'])
        elif part['type'] == 'image':
            img_info = part['content']
            try:
                image = Image.open(img_info['path'])
                caption = ""
                if img_info.get('alt'):
                    caption = f"üì∑ {img_info['alt']}"
                elif img_info.get('title'):
                    caption = f"üì∑ {img_info['title']}"
                else:
                    caption = f"üì∑ H√¨nh ·∫£nh t·ª´ {img_info['source_doc']['title']}"

                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    st.image(image, caption=caption, use_container_width=True)
                
                st.markdown("<br>", unsafe_allow_html=True)
            except Exception as e:
                st.error(f"Kh√¥ng th·ªÉ hi·ªÉn th·ªã ·∫£nh: {e}")

def auto_embed_images_in_answer(answer, relevant_images):
    """T·ª± ƒë·ªông ch√®n ·∫£nh v√†o c√¢u tr·∫£ l·ªùi"""
    paragraphs = [p.strip() for p in answer.split('\n\n') if p.strip()]
    
    if len(paragraphs) <= 1:
        sentences = [s.strip() + '.' for s in answer.split('.') if s.strip()]
        paragraphs = sentences

    total_parts = len(paragraphs)
    total_images = len(relevant_images)

    if total_images == 0:
        st.markdown(answer)
        return

    insert_positions = []
    if total_parts > 1:
        step = max(1, total_parts // (total_images + 1))
        for i in range(min(total_images, total_parts - 1)):
            pos = (i + 1) * step
            if pos < total_parts:
                insert_positions.append(pos)

    image_index = 0
    for i, paragraph in enumerate(paragraphs):
        st.markdown(paragraph)
        
        if i in insert_positions and image_index < len(relevant_images):
            img_info = relevant_images[image_index]
            try:
                image = Image.open(img_info['path'])
                caption = ""
                if img_info.get('alt'):
                    caption = f"üì∑ {img_info['alt']}"
                elif img_info.get('title'):
                    caption = f"üì∑ {img_info['title']}"
                else:
                    caption = f"üì∑ H√¨nh ·∫£nh minh h·ªça"

                st.markdown("<br>", unsafe_allow_html=True)
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    st.image(image, caption=caption, use_container_width=True)
                st.markdown("<br>", unsafe_allow_html=True)
                
                image_index += 1
            except Exception as e:
                st.error(f"Kh√¥ng th·ªÉ hi·ªÉn th·ªã ·∫£nh: {e}")

def collect_single_url(url):
    """Thu th·∫≠p d·ªØ li·ªáu t·ª´ m·ªôt URL"""
    try:
        scraper = WebScraperLocal(overwrite=False)
        success = scraper.fetch_and_save(url)
        return success, scraper.success_count, scraper.error_count
    except Exception as e:
        return False, 0, 1

def process_question(question):
    """X·ª≠ l√Ω c√¢u h·ªèi v√† hi·ªÉn th·ªã k·∫øt qu·∫£"""
    with st.spinner("üîç ƒêang t√¨m ki·∫øm v√† ph√¢n t√≠ch..."):
        try:
            # Kh·ªüi t·∫°o AI client
            config = load_config_local()
            client = LocalGemmaClient(config["LOCAL_MODEL"])

            # T√¨m ki·∫øm documents li√™n quan
            relevant_docs, similarity = st.session_state.rag_system.enhanced_search_multimodal(
                question, threshold=0.3, top_k=3
            )

            # Ki·ªÉm tra xem c√≥ n√™n s·ª≠ d·ª•ng ki·∫øn th·ª©c chung kh√¥ng
            use_general = should_use_general_knowledge(relevant_docs, similarity, question)

            if use_general:
                # Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan - tr·∫£ l·ªùi b·∫±ng ki·∫øn th·ª©c chung
                st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong database. AI s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung.")
                
                general_context = create_general_knowledge_context(question)
                
                answer = client.generate_response(
                    prompt=question,
                    context=general_context,
                    max_tokens=2000
                )

                # L∆∞u v√†o l·ªãch s·ª≠ v·ªõi th√¥ng tin ƒë·∫∑c bi·ªát
                st.session_state.chat_history.append((question, answer, [], "general"))
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                st.markdown(f"**üôã B·∫°n:** {question}")
                st.markdown("**ü§ñ AI (Ki·∫øn th·ª©c chung):**")
                st.markdown(answer)
                
                # Hi·ªÉn th·ªã th√¥ng tin debug
                with st.expander("üîç Th√¥ng tin t√¨m ki·∫øm"):
                    st.write(f"**ƒê·ªô t∆∞∆°ng ƒë·ªìng:** {similarity:.3f} (th·∫•p h∆°n ng∆∞·ª°ng)")
                    st.write(f"**S·ªë documents t√¨m th·∫•y:** {len(relevant_docs)}")
                    st.write("**Tr·∫°ng th√°i:** Tr·∫£ l·ªùi b·∫±ng ki·∫øn th·ª©c chung")

            else:
                # T√¨m th·∫•y th√¥ng tin li√™n quan - x·ª≠ l√Ω nh∆∞ b√¨nh th∆∞·ªùng
                # T·∫°o context t·ª´ documents
                text_context = ""
                for doc in relevant_docs:
                    text_context += f"\n\n--- {doc['title']} ---\n"
                    text_context += f"{doc['description']}\n"
                    text_context += f"{doc['text_content'][:1000]}..."

                # L·∫•y ·∫£nh li√™n quan
                relevant_images = st.session_state.rag_system.get_relevant_images_for_context(
                    relevant_docs, question, max_images=5
                )

                # T·∫°o context ƒëa ph∆∞∆°ng ti·ªán
                multimodal_context = create_intelligent_multimodal_context(
                    text_context, relevant_images, question
                )

                # G·ªçi AI ƒë·ªÉ tr·∫£ l·ªùi
                answer = client.generate_response(
                    prompt=question,
                    context=multimodal_context,
                    max_tokens=2000
                )

                # L∆∞u v√†o l·ªãch s·ª≠
                st.session_state.chat_history.append((question, answer, relevant_images, "rag"))
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                st.markdown(f"**üôã B·∫°n:** {question}")
                st.markdown("**ü§ñ AI:**")
                smart_display_answer_with_embedded_images(answer, relevant_images)
                
                # Hi·ªÉn th·ªã th√¥ng tin debug
                with st.expander("üîç Th√¥ng tin t√¨m ki·∫øm"):
                    st.write(f"**ƒê·ªô t∆∞∆°ng ƒë·ªìng:** {similarity:.3f}")
                    st.write(f"**S·ªë documents t√¨m th·∫•y:** {len(relevant_docs)}")
                    st.write(f"**S·ªë ·∫£nh li√™n quan:** {len(relevant_images)}")
                    
                    for i, doc in enumerate(relevant_docs, 1):
                        st.write(f"**{i}.** {doc['title']}")

        except Exception as e:
            st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")
            logger.error(f"L·ªói process_question: {e}")
            traceback.print_exc()

def main():
    st.set_page_config(
        page_title="ü§ñ Multimodal RAG Local v·ªõi Gemma",
        page_icon="ü§ñ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # Kh·ªüi t·∫°o session state
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = EnhancedMultimodalRAGLocal("../db")
    
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    # Load config
    config = load_config_local()
    base_url = config["LOCAL_MODEL"]["base_url"]
    current_model = config["LOCAL_MODEL"]["model"]

    # Sidebar v·ªõi URL Collector v√† th√¥ng tin Ollama
    with st.sidebar:
        st.markdown("## üåê Thu th·∫≠p d·ªØ li·ªáu")
        
        # √î nh·∫≠p URL
        new_url = st.text_input(
            "Nh·∫≠p URL ƒë·ªÉ thu th·∫≠p:",
            placeholder="https://example.com/article",
            help="Nh·∫≠p URL c·ªßa trang web b·∫°n mu·ªën thu th·∫≠p d·ªØ li·ªáu"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ Thu th·∫≠p", use_container_width=True):
                if new_url.strip():
                    with st.spinner("ƒêang thu th·∫≠p d·ªØ li·ªáu..."):
                        success, success_count, error_count = collect_single_url(new_url.strip())
                        
                        if success:
                            st.success(f"‚úÖ Thu th·∫≠p th√†nh c√¥ng!")
                            # Reload d·ªØ li·ªáu
                            st.session_state.rag_system.load_multimodal_data()
                            st.rerun()
                        else:
                            st.error(f"‚ùå L·ªói thu th·∫≠p d·ªØ li·ªáu")
                else:
                    st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p URL")
        
        with col2:
            if st.button("üîÑ Reload DB", use_container_width=True):
                with st.spinner("ƒêang t·∫£i l·∫°i d·ªØ li·ªáu..."):
                    st.session_state.rag_system.load_multimodal_data()
                    st.success("‚úÖ ƒê√£ t·∫£i l·∫°i database")
                    st.rerun()

        st.markdown("---")

        # Th√¥ng tin Ollama Server
        st.markdown("## üîó Ollama Server")
        
        # Ki·ªÉm tra k·∫øt n·ªëi
        is_connected, status_msg, response_text = check_ollama_connection(base_url)
        
        if is_connected:
            st.success(f"‚úÖ {status_msg}")
            st.caption(f"üåê {base_url}")
        else:
            st.error(f"‚ùå {status_msg}")
            st.caption(f"üåê {base_url}")
            
        # Hi·ªÉn th·ªã th√¥ng tin models
        if is_connected:
            with st.expander("ü§ñ Th√¥ng tin Models", expanded=True):
                # Model hi·ªán t·∫°i
                st.markdown(f"**Model ƒëang d√πng:** `{current_model}`")
                
                # L·∫•y th√¥ng tin chi ti·∫øt model hi·ªán t·∫°i
                success, model_info = get_model_info(base_url, current_model)
                if success:
                    if 'details' in model_info:
                        details = model_info['details']
                        st.write(f"**K√≠ch th∆∞·ªõc:** {format_model_size(details.get('size'))}")
                        st.write(f"**Format:** {details.get('format', 'N/A')}")
                        st.write(f"**Family:** {details.get('family', 'N/A')}")
                        if 'parameter_size' in details:
                            st.write(f"**Parameters:** {details['parameter_size']}")
                
                # Models ƒëang ch·∫°y
                success, running_models = get_running_models(base_url)
                if success and running_models:
                    st.markdown("**üü¢ Models ƒëang ch·∫°y:**")
                    for model in running_models:
                        name = model.get('name', 'Unknown')
                        size = format_model_size(model.get('size'))
                        expires_at = model.get('expires_at')
                        
                        st.write(f"‚Ä¢ `{name}` ({size})")
                        if expires_at:
                            expire_time = format_duration(expires_at)
                            st.caption(f"  Expires: {expire_time}")
                else:
                    st.write("**üî¥ Kh√¥ng c√≥ model n√†o ƒëang ch·∫°y**")
                
                # T·∫•t c·∫£ models c√≥ s·∫µn
                success, all_models = get_ollama_models(base_url)
                if success and all_models:
                    st.markdown("**üì¶ Models c√≥ s·∫µn:**")
                    for model in all_models[:5]:  # Hi·ªÉn th·ªã t·ªëi ƒëa 5 models
                        name = model.get('name', 'Unknown')
                        size = format_model_size(model.get('size'))
                        modified = model.get('modified_at', '')
                        
                        if modified:
                            try:
                                mod_date = datetime.fromisoformat(modified.replace('Z', '+00:00'))
                                mod_str = mod_date.strftime('%d/%m/%Y')
                            except:
                                mod_str = modified[:10]
                        else:
                            mod_str = 'N/A'
                        
                        st.write(f"‚Ä¢ `{name}` ({size})")
                        st.caption(f"  Modified: {mod_str}")
                    
                    if len(all_models) > 5:
                        st.caption(f"... v√† {len(all_models) - 5} models kh√°c")

        st.markdown("---")

        # Th√¥ng tin Database
        st.markdown("## üìä Th√¥ng tin Database")
        
        if st.session_state.rag_system.has_data:
            db_stats = st.session_state.rag_system.get_database_statistics()
            
            # Metrics t·ªïng quan
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Documents", db_stats['total_documents'])
                st.metric("H√¨nh ·∫£nh", db_stats['total_images'])
            with col2:
                text_kb = db_stats['total_text_length'] / 1024
                st.metric("Text", f"{text_kb:.1f} KB")
                
                # T√≠nh t·ªïng k√≠ch th∆∞·ªõc
                total_size = sum(site['size_bytes'] for site in db_stats['sites_info'])
                st.metric("T·ªïng k√≠ch th∆∞·ªõc", format_model_size(total_size))
            
            # Chi ti·∫øt t·ª´ng site
            with st.expander("üìö Chi ti·∫øt Documents"):
                for i, site in enumerate(db_stats['sites_info'], 1):
                    st.write(f"**{i}.** {site['name']}")
                    st.caption(f"üîó {site['url']}")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.caption(f"üñºÔ∏è {site['images']} ·∫£nh")
                    with col2:
                        st.caption(f"üìÑ {site['text_length']} k√Ω t·ª±")
                    with col3:
                        st.caption(f"üíæ {format_model_size(site['size_bytes'])}")
                    
                    # Th·ªùi gian thu th·∫≠p
                    scraped_at = site['scraped_at']
                    if scraped_at != 'N/A':
                        try:
                            scraped_date = datetime.fromisoformat(scraped_at.replace('Z', '+00:00'))
                            scraped_str = scraped_date.strftime('%d/%m/%Y %H:%M')
                            st.caption(f"‚è∞ Thu th·∫≠p: {scraped_str}")
                        except:
                            st.caption(f"‚è∞ Thu th·∫≠p: {scraped_at[:19]}")
                    
                    st.markdown("---")
        else:
            st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu trong database")
            st.info("üí° H√£y thu th·∫≠p d·ªØ li·ªáu t·ª´ URL ho·∫∑c ch·∫°y collect_local.py")

        # Th·ªëng k√™ t√¨m ki·∫øm
        stats = st.session_state.rag_system.get_search_statistics()
        if stats:
            st.markdown("### üìà Th·ªëng k√™ T√¨m ki·∫øm")
            st.metric("T·ªïng truy v·∫•n", stats['total_queries'])
            st.metric("T·ª∑ l·ªá th√†nh c√¥ng", f"{stats['success_rate']:.1f}%")
            st.metric("ƒê·ªô t∆∞∆°ng ƒë·ªìng TB", f"{stats['average_similarity']:.3f}")
            
            # Intent distribution
            if stats['intent_distribution']:
                st.markdown("**Ph√¢n b·ªë Intent:**")
                for intent, count in stats['intent_distribution'].items():
                    percentage = (count / stats['total_queries']) * 100
                    st.caption(f"‚Ä¢ {intent}: {count} ({percentage:.1f}%)")

    # Main content
    st.title("ü§ñ Multimodal RAG Local v·ªõi Gemma")
    st.markdown("*H·ªá th·ªëng t√¨m ki·∫øm v√† tr·∫£ l·ªùi th√¥ng minh v·ªõi h√¨nh ·∫£nh*")

    # Hi·ªÉn th·ªã tr·∫°ng th√°i k·∫øt n·ªëi
    col1, col2 = st.columns([3, 1])
    with col1:
        if is_connected:
            st.success(f"üîó K·∫øt n·ªëi Ollama: **{current_model}** t·∫°i {base_url}")
        else:
            st.error(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi Ollama t·∫°i {base_url}")
    
    with col2:
        if st.button("üîÑ Ki·ªÉm tra l·∫°i", use_container_width=True):
            st.rerun()

    # Th√¥ng b√°o v·ªÅ ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông
    if not st.session_state.rag_system.has_data:
        st.info("üß† **Ch·∫ø ƒë·ªô AI Chung**: Kh√¥ng c√≥ database, AI s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung")
    else:
        st.success("üîç **Ch·∫ø ƒë·ªô RAG**: AI s·∫Ω t√¨m ki·∫øm trong database tr∆∞·ªõc, n·∫øu kh√¥ng c√≥ s·∫Ω d√πng ki·∫øn th·ª©c chung")

    # Chat interface
    st.markdown("## üí¨ Tr√≤ chuy·ªán v·ªõi AI")

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for i, chat_item in enumerate(st.session_state.chat_history):
        if len(chat_item) == 4:  # ƒê·ªãnh d·∫°ng m·ªõi v·ªõi mode
            question, answer, images, mode = chat_item
        else:  # ƒê·ªãnh d·∫°ng c≈©
            question, answer, images = chat_item
            mode = "rag" if images else "general"
            
        with st.container():
            st.markdown(f"**üôã B·∫°n:** {question}")
            if mode == "general":
                st.markdown("**ü§ñ AI (Ki·∫øn th·ª©c chung):**")
                st.markdown(answer)
            else:
                st.markdown("**ü§ñ AI:**")
                smart_display_answer_with_embedded_images(answer, images)
            st.markdown("---")

    # Input cho c√¢u h·ªèi m·ªõi
    question = st.text_input(
        "ƒê·∫∑t c√¢u h·ªèi c·ªßa b·∫°n:",
        placeholder="V√≠ d·ª•: Python l√† g√¨? Machine Learning ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?...",
        key="question_input"
    )

    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        if st.button("üöÄ G·ª≠i c√¢u h·ªèi", use_container_width=True) and question:
            if is_connected:
                process_question(question)
            else:
                st.error("‚ùå Kh√¥ng th·ªÉ g·ª≠i c√¢u h·ªèi. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi Ollama.")
    
    with col2:
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠", use_container_width=True):
            st.session_state.chat_history = []
            st.rerun()
    
    with col3:
        threshold = st.slider("üéØ Ng∆∞·ª°ng t√¨m ki·∫øm", 0.1, 0.8, 0.3, 0.1)

if __name__ == "__main__":
    main()

import os
import json
import streamlit as st
from pathlib import Path
from PIL import Image
import base64
from io import BytesIO
import traceback
import numpy as np
import re
import logging
import time
import shutil
import requests

try:
    import torch
    if not hasattr(torch.classes, '__path__'):
        torch.classes.__path__ = [os.path.join(torch.__path__[0], 'classes')]
except ImportError:
    pass
except Exception as e:
    print(f"Warning: Could not fix torch.classes path: {e}")

from config_local import load_config_local
from rag_local import (
    load_documents, search_documents, ask_local_model,
    search_documents_with_threshold, adaptive_threshold_search,
    enhanced_search_with_metadata
)
from collect_local import WebScraperLocal
from local_gemma import LocalGemmaClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def validate_query_input(question, config):
    """Validate input query c∆° b·∫£n"""
    validation_config = config.get('QUERY_VALIDATION', {})
    min_length = validation_config.get('min_query_length', 1)
    
    if len(question.strip()) < min_length:
        return False, f"C√¢u h·ªèi qu√° ng·∫Øn. Vui l√≤ng nh·∫≠p √≠t nh·∫•t {min_length} k√Ω t·ª±."
    
    return True, "Query h·ª£p l·ªá"

class EnhancedMultimodalRAGLocal:
    def __init__(self, db_dir):
        self.db_dir = Path(db_dir)
        self.documents = []
        self.has_data = False
        self.query_history = []
        self.load_multimodal_data()

    def load_multimodal_data(self):
        """Load d·ªØ li·ªáu ƒëa ph∆∞∆°ng ti·ªán t·ª´ database"""
        self.documents = []
        logger.info(f"Ki·ªÉm tra th∆∞ m·ª•c db: {self.db_dir.absolute()}")
        
        if not self.db_dir.exists():
            logger.warning(f"Th∆∞ m·ª•c db kh√¥ng t·ªìn t·∫°i: {self.db_dir}")
            self.has_data = False
            return

        site_dirs = [d for d in self.db_dir.iterdir() if d.is_dir()]
        for site_dir in site_dirs:
            metadata_file = site_dir / "metadata.json"
            if not metadata_file.exists():
                continue

            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                content_file = site_dir / "content.txt"
                text_content = ""
                if content_file.exists():
                    with open(content_file, 'r', encoding='utf-8') as f:
                        text_content = f.read()

                doc = {
                    'site_name': site_dir.name,
                    'url': data['metadata']['url'],
                    'title': data['metadata']['title'],
                    'description': data['metadata']['description'],
                    'text_content': text_content,
                    'images': data.get('images', []),
                    'image_dir': site_dir / "images",
                    'metadata': data['metadata']
                }

                self.documents.append(doc)
            except Exception as e:
                logger.error(f"L·ªói load d·ªØ li·ªáu t·ª´ {site_dir}: {e}")

        self.has_data = len(self.documents) > 0

    def get_database_info(self):
        """L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ database"""
        db_info = {
            'total_documents': len(self.documents),
            'total_words': 0,
            'total_images': 0,
            'content_types': {},
            'keywords': set(),
            'titles': [],
            'urls': [],
            'sample_content': []
        }

        for doc in self.documents:
            word_count = doc['metadata'].get('word_count', 0)
            db_info['total_words'] += word_count
            db_info['total_images'] += len(doc['images'])
            content_type = doc['metadata'].get('content_type', 'unknown')
            db_info['content_types'][content_type] = db_info['content_types'].get(content_type, 0) + 1
            db_info['titles'].append(doc['title'])
            db_info['urls'].append(doc['url'])
            if doc['text_content']:
                sample = doc['text_content'][:200] + "..."
                db_info['sample_content'].append({
                    'title': doc['title'],
                    'sample': sample
                })

        return db_info

    def enhanced_search_multimodal(self, query, threshold=None, top_k=3):
        """T√¨m ki·∫øm v·ªõi threshold t·ª´ config"""
        if not self.has_data:
            return [], 0

        config = load_config_local()
        threshold = threshold or config.get('DB_THRESHOLD', 0.3)

        relevant_docs, max_similarity = enhanced_search_with_metadata(
            query, self.documents, threshold, top_k
        )

        if not relevant_docs and threshold > 0.2:
            logger.info(f"Kh√¥ng t√¨m th·∫•y v·ªõi threshold {threshold}, th·ª≠ v·ªõi 0.2")
            relevant_docs, max_similarity = enhanced_search_with_metadata(
                query, self.documents, 0.2, top_k
            )

        self.query_history.append({
            'query': query,
            'similarity': max_similarity,
            'results_count': len(relevant_docs),
            'timestamp': time.time(),
            'threshold_used': threshold
        })

        return relevant_docs, max_similarity

    def get_relevant_images_for_context(self, relevant_docs, query, max_images=5):
        """L·∫•y ·∫£nh li√™n quan v·ªõi scoring c·∫£i ti·∫øn"""
        relevant_images = []
        query_lower = query.lower()
        query_words = set(query_lower.split())

        for doc in relevant_docs:
            for img_info in doc['images']:
                relevance_score = 0
                alt_text = img_info.get('alt', '').lower()
                title_text = img_info.get('title', '').lower()

                alt_words = set(alt_text.split())
                title_words = set(title_text.split())

                alt_matches = len(query_words.intersection(alt_words))
                title_matches = len(query_words.intersection(title_words))
                relevance_score += (alt_matches * 2) + (title_matches * 2)

                for word in query_words:
                    if len(word) > 3:
                        if word in alt_text:
                            relevance_score += 1
                        if word in title_text:
                            relevance_score += 1

                if len(alt_text) > 20 or len(title_text) > 20:
                    relevance_score += 0.5

                img_path = Path(img_info.get('local_path', ''))
                if img_path.exists() and (relevance_score > 0 or (not alt_text and not title_text)):
                    relevant_images.append({
                        'path': img_path,
                        'alt': img_info.get('alt', ''),
                        'title': img_info.get('title', ''),
                        'url': img_info.get('url', ''),
                        'source_doc': doc,
                        'relevance_score': relevance_score
                    })

        unique_images = []
        seen_paths = set()
        for img in sorted(relevant_images, key=lambda x: x['relevance_score'], reverse=True):
            if img['path'] not in seen_paths:
                unique_images.append(img)
                seen_paths.add(img['path'])

        return unique_images[:max_images]

def display_database_debug_info(rag_system):
    """Hi·ªÉn th·ªã th√¥ng tin debug chi ti·∫øt v·ªÅ database"""
    st.subheader("üîç Th√¥ng tin chi ti·∫øt Database")
    db_info = rag_system.get_database_info()

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Documents", db_info['total_documents'])
    with col2:
        st.metric("T·ªïng t·ª´", f"{db_info['total_words']:,}")
    with col3:
        st.metric("T·ªïng ·∫£nh", db_info['total_images'])
    with col4:
        st.metric("Lo·∫°i n·ªôi dung", len(db_info['content_types']))

    if db_info['content_types']:
        st.write("**Ph√¢n lo·∫°i n·ªôi dung:**")
        for content_type, count in db_info['content_types'].items():
            st.write(f"- {content_type}: {count} documents")

    with st.expander("üìã Danh s√°ch Documents", expanded=False):
        for i, title in enumerate(db_info['titles'], 1):
            st.write(f"{i}. **{title}**")
            if i-1 < len(db_info['urls']):
                st.caption(f"URL: {db_info['urls'][i-1]}")

    with st.expander("üìÑ M·∫´u n·ªôi dung", expanded=False):
        for sample in db_info['sample_content'][:3]:
            st.write(f"**{sample['title']}**")
            st.write(sample['sample'])
            st.write("---")

def get_server_info(config):
    """L·∫•y th√¥ng tin server model API"""
    server_info = {
        'status': 'unknown',
        'base_url': config.get('LOCAL_MODEL', {}).get('base_url', 'N/A'),
        'model': config.get('LOCAL_MODEL', {}).get('model', 'N/A'),
        'connection_status': False,
        'response_time': None
    }

    try:
        base_url = server_info['base_url']
        if not base_url or base_url == 'N/A':
            server_info['status'] = 'No URL configured'
            return server_info

        local_client = LocalGemmaClient(base_url=base_url, model=server_info['model'])
        start_time = time.time()
        connection_status = local_client.check_connection()
        response_time = time.time() - start_time

        server_info['connection_status'] = connection_status
        server_info['response_time'] = response_time
        server_info['status'] = 'Connected' if connection_status else 'Connection Failed'

    except Exception as e:
        server_info['status'] = f'Error: {str(e)}'
        logger.error(f"L·ªói khi l·∫•y th√¥ng tin server: {e}")

    return server_info

def display_server_info():
    """Hi·ªÉn th·ªã th√¥ng tin server trong sidebar"""
    config = load_config_local()
    server_info = get_server_info(config)

    st.sidebar.header("üñ•Ô∏è Th√¥ng tin Server")
    status = server_info['status']
    if server_info['connection_status']:
        st.sidebar.success(f"‚úÖ {status}")
    elif 'Error' in status or 'Failed' in status:
        st.sidebar.error(f"‚ùå {status}")
    else:
        st.sidebar.warning(f"‚ö†Ô∏è {status}")

    with st.sidebar.expander("Chi ti·∫øt Server", expanded=False):
        st.write(f"**URL:** {server_info['base_url']}")
        if server_info['response_time']:
            st.write(f"**Th·ªùi gian ph·∫£n h·ªìi:** {server_info['response_time']:.3f}s")
        st.write(f"**Model hi·ªán t·∫°i:** {server_info['model']}")

def create_sidebar():
    """T·∫°o sidebar v·ªõi c√°c ch·ª©c nƒÉng ƒëi·ªÅu khi·ªÉn"""
    st.sidebar.title("üîß ƒêi·ªÅu khi·ªÉn h·ªá th·ªëng")
    display_server_info()

    st.sidebar.header("üì• Thu th·∫≠p d·ªØ li·ªáu m·ªõi")
    with st.sidebar.expander("Th√™m URL m·ªõi", expanded=False):
        new_url = st.text_input(
            "Nh·∫≠p URL c·∫ßn thu th·∫≠p:",
            placeholder="https://example.com/article",
            key="new_url_input"
        )

        if st.button("Thu th·∫≠p", key="collect_single", type="primary"):
            if new_url:
                collect_single_url(new_url)
            else:
                st.error("Vui l√≤ng nh·∫≠p URL")

def collect_single_url(url):
    """Thu th·∫≠p d·ªØ li·ªáu t·ª´ m·ªôt URL"""
    try:
        if not url.startswith(('http://', 'https://')):
            st.error("URL ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng http:// ho·∫∑c https://")
            return

        with st.spinner(f"ƒêang thu th·∫≠p t·ª´ {url}..."):
            scraper = WebScraperLocal(overwrite=False)
            success = scraper.fetch_and_save(url)

            if success:
                st.success(f"‚úÖ Thu th·∫≠p th√†nh c√¥ng t·ª´ {url}")
                if 'rag_system' in st.session_state:
                    st.session_state.rag_system.load_multimodal_data()
                st.rerun()
            else:
                st.error(f"‚ùå Kh√¥ng th·ªÉ thu th·∫≠p t·ª´ {url}")

    except Exception as e:
        st.error(f"L·ªói: {e}")
        logger.error(f"L·ªói thu th·∫≠p {url}: {e}")

def handle_no_results_fallback(question, config):
    """X·ª≠ l√Ω fallback khi kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£"""
    st.info("üîç Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong database. ƒêang g·ªçi AI ƒë·ªÉ tr·∫£ l·ªùi...")
    
    fallback_context = f"""
C√¢u h·ªèi: {question}

Th√¥ng tin: Kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ trong c∆° s·ªü d·ªØ li·ªáu.

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
2. D·ª±a tr√™n ki·∫øn th·ª©c chung
3. N·∫øu kh√¥ng bi·∫øt, h√£y n√≥i r√µ v√† ƒë·ªÅ xu·∫•t t√¨m ki·∫øm th√™m
4. Tr·∫£ l·ªùi chi ti·∫øt v√† h·ªØu √≠ch

Tr·∫£ l·ªùi:
"""
    
    try:
        response = ask_local_model(question, fallback_context, config)
        st.info("üí° **L∆∞u √Ω:** C√¢u tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa AI.")
        return response
    except Exception as e:
        logger.error(f"Error in fallback AI response: {e}")
        return f"Xin l·ªói, kh√¥ng th·ªÉ tr·∫£ l·ªùi do l·ªói: {e}"

def create_intelligent_multimodal_context(text_context, relevant_images, question):
    """T·∫°o context ƒëa ph∆∞∆°ng ti·ªán th√¥ng minh"""
    context = f"Th√¥ng tin vƒÉn b·∫£n:\n{text_context}\n\n"
    
    if relevant_images:
        context += "Th√¥ng tin h√¨nh ·∫£nh:\n"
        for i, img_info in enumerate(relevant_images, 1):
            context += f"\n[IMAGE_{i}]:"
            if img_info['alt']:
                context += f" M√¥ t·∫£: {img_info['alt']}"
            if img_info['title']:
                context += f" Ti√™u ƒë·ªÅ: {img_info['title']}"
            context += f" (Ngu·ªìn: {img_info['source_doc']['title']})"

    context += f"""

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1. Tr·∫£ l·ªùi chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát
2. S·ª≠ d·ª•ng [IMAGE_X] ƒë·ªÉ ch·ªâ ƒë·ªãnh ·∫£nh minh h·ªça
3. D·ª±a tr√™n th√¥ng tin vƒÉn b·∫£n v√† h√¨nh ·∫£nh
4. N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan, n√≥i r√µ
5. Tr·∫£ l·ªùi c√¢u h·ªèi: {question}
"""
    return context

def parse_answer_with_image_markers(answer, relevant_images):
    """Ph√¢n t√≠ch c√¢u tr·∫£ l·ªùi v√† t√°ch marker ·∫£nh"""
    image_pattern = r'\[IMAGE_(\d+)\]'
    parts = re.split(image_pattern, answer)
    content_parts = []

    for i, part in enumerate(parts):
        if part.isdigit():
            image_index = int(part) - 1
            if 0 <= image_index < len(relevant_images):
                content_parts.append({
                    'type': 'image',
                    'content': relevant_images[image_index],
                    'index': image_index
                })
        else:
            if part.strip():
                content_parts.append({
                    'type': 'text',
                    'content': part.strip()
                })

    return content_parts

def smart_display_answer_with_embedded_images(answer, relevant_images):
    """Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi v·ªõi ·∫£nh ch√®n th√¥ng minh"""
    if not relevant_images:
        st.markdown(answer)
        return

    content_parts = parse_answer_with_image_markers(answer, relevant_images)

    if not any(part['type'] == 'image' for part in content_parts):
        auto_embed_images_in_answer(answer, relevant_images)
        return

    for part in content_parts:
        if part['type'] == 'text':
            st.markdown(part['content'])
        elif part['type'] == 'image':
            img_info = part['content']
            try:
                image = Image.open(img_info['path'])
                caption = img_info.get('alt') or img_info.get('title') or f"H√¨nh ·∫£nh t·ª´ {img_info['source_doc']['title']}"
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    st.image(image, caption=caption, use_container_width=True)
                st.markdown("", unsafe_allow_html=True)
            except Exception as e:
                st.error(f"Kh√¥ng th·ªÉ hi·ªÉn th·ªã ·∫£nh: {e}")

def auto_embed_images_in_answer(answer, relevant_images):
    """T·ª± ƒë·ªông ch√®n ·∫£nh v√†o c√¢u tr·∫£ l·ªùi"""
    paragraphs = [p.strip() for p in answer.split('\n\n') if p.strip()]
    total_images = len(relevant_images)

    if total_images == 0:
        st.markdown(answer)
        return

    insert_positions = []
    if len(paragraphs) > 1:
        step = max(1, len(paragraphs) // (total_images + 1))
        for i in range(min(total_images, len(paragraphs) - 1)):
            pos = (i + 1) * step
            if pos < len(paragraphs):
                insert_positions.append(pos)

    image_index = 0
    for i, paragraph in enumerate(paragraphs):
        st.markdown(paragraph)
        if i in insert_positions and image_index < len(relevant_images):
            img_info = relevant_images[image_index]
            try:
                image = Image.open(img_info['path'])
                caption = img_info.get('alt') or img_info.get('title') or "H√¨nh ·∫£nh minh h·ªça"
                st.markdown("", unsafe_allow_html=True)
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    st.image(image, caption=caption, use_container_width=True)
                st.markdown("", unsafe_allow_html=True)
                image_index += 1
            except Exception as e:
                st.error(f"Kh√¥ng th·ªÉ hi·ªÉn th·ªã ·∫£nh: {e}")

def main():
    """H√†m ch√≠nh c·ªßa ·ª©ng d·ª•ng"""
    st.set_page_config(
        page_title="RAG Local v·ªõi ·∫¢nh Ch√®n",
        page_icon="ü§ñ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    create_sidebar()

    st.title("ü§ñ H·ªá th·ªëng RAG Local v·ªõi ·∫¢nh Ch√®n Th√¥ng Minh")
    st.markdown("---")

    if 'rag_system' not in st.session_state:
        with st.spinner("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng..."):
            st.session_state.rag_system = EnhancedMultimodalRAGLocal("db")

    rag_system = st.session_state.rag_system

    if not rag_system.has_data:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng thu th·∫≠p d·ªØ li·ªáu t·ª´ sidebar.")
        col1, col2 = st.columns(2)
        with col1:
            st.info("üí° **H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:**")
            st.write("1. S·ª≠ d·ª•ng sidebar ƒë·ªÉ th√™m URL")
            st.write("2. Nh·∫•n 'Thu th·∫≠p' ƒë·ªÉ t·∫£i d·ªØ li·ªáu")
            st.write("3. ƒê·∫∑t c√¢u h·ªèi sau khi c√≥ d·ªØ li·ªáu")
        with col2:
            st.info("üîß **T√≠nh nƒÉng:**")
            st.write("- Thu th·∫≠p t·ª´ URL")
            st.write("- ·∫¢nh ch√®n th√¥ng minh")
            st.write("- AI tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi")

    if rag_system.has_data:
        display_database_debug_info(rag_system)
        st.markdown("---")

    st.header("üí¨ H·ªèi ƒë√°p th√¥ng minh v·ªõi AI")
    question = st.text_input(
        "ƒê·∫∑t c√¢u h·ªèi:",
        placeholder="B·∫°n c√≥ th·ªÉ h·ªèi b·∫•t k·ª≥ ƒëi·ªÅu g√¨...",
        help="Nh·∫≠p c√¢u h·ªèi v√† nh·∫•n Enter."
    )

    with st.expander("‚öôÔ∏è T√πy ch·ªçn t√¨m ki·∫øm", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            threshold = st.slider(
                "Ng∆∞·ª°ng t√¨m ki·∫øm:",
                min_value=0.3,
                max_value=0.5,
                value=0.3,
                step=0.05
            )
        with col2:
            max_results = st.selectbox(
                "S·ªë k·∫øt qu·∫£ t·ªëi ƒëa:",
                options=[1, 2, 3, 4, 5],
                index=2
            )

    if question:
        config = load_config_local()
        is_valid, validation_message = validate_query_input(question, config)
        if not is_valid:
            st.error(f"‚ùå {validation_message}")
            return

        with st.spinner("üîç ƒêang t√¨m ki·∫øm v√† t·∫°o c√¢u tr·∫£ l·ªùi..."):
            try:
                st.info(f"üîç ƒêang t√¨m ki·∫øm: '{question}' v·ªõi threshold {threshold}")

                relevant_docs, similarity = rag_system.enhanced_search_multimodal(
                    question, threshold, max_results
                )

                st.subheader("üîç K·∫øt qu·∫£ t√¨m ki·∫øm")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ƒê·ªô t∆∞∆°ng t·ª± t·ªëi ƒëa", f"{similarity:.3f}")
                with col2:
                    st.metric("S·ªë documents", len(relevant_docs))
                with col3:
                    st.metric("Ng∆∞·ª°ng s·ª≠ d·ª•ng", f"{threshold:.3f}")
                with col4:
                    st.metric("Ng∆∞·ª°ng t·ªëi thi·ªÉu", f"{config.get('MIN_SIMILARITY_THRESHOLD', 0.5):.3f}")

                if relevant_docs and similarity >= config.get('MIN_SIMILARITY_THRESHOLD', 0.5):
                    with st.expander("üìã Documents ƒë∆∞·ª£c t√¨m th·∫•y", expanded=True):
                        for i, doc in enumerate(relevant_docs, 1):
                            st.write(f"**{i}. {doc['title']}**")
                            st.caption(f"URL: {doc['url']}")
                            st.caption(f"Lo·∫°i: {doc['metadata'].get('content_type', 'unknown')}")
                            snippet = doc['text_content'][:300] + "..."
                            highlighted_snippet = snippet
                            for word in question.split():
                                if len(word) > 3:
                                    highlighted_snippet = highlighted_snippet.replace(
                                        word, f"**{word}**"
                                    )
                            st.write(f"*ƒêo·∫°n tr√≠ch:* {highlighted_snippet}")
                            st.write("---")

                    text_context = "\n\n".join([
                        f"Ti√™u ƒë·ªÅ: {doc['title']}\nM√¥ t·∫£: {doc['description']}\nN·ªôi dung: {doc['text_content'][:1000]}..."
                        for doc in relevant_docs
                    ])

                    relevant_images = rag_system.get_relevant_images_for_context(
                        relevant_docs, question, 5
                    )

                    context = create_intelligent_multimodal_context(
                        text_context, relevant_images, question
                    )

                    answer = ask_local_model(question, context, config)

                    st.subheader("üìù C√¢u tr·∫£ l·ªùi:")
                    st.success("‚úÖ **D·ª±a tr√™n database**")
                    smart_display_answer_with_embedded_images(answer, relevant_images)

                    if relevant_images:
                        with st.expander("üñºÔ∏è Th√¥ng tin ·∫£nh", expanded=False):
                            for i, img_info in enumerate(relevant_images, 1):
                                st.write(f"**·∫¢nh {i}:**")
                                st.write(f"- Ngu·ªìn: {img_info['source_doc']['title']}")
                                if img_info['alt']:
                                    st.write(f"- M√¥ t·∫£: {img_info['alt']}")
                                if img_info['title']:
                                    st.write(f"- Ti√™u ƒë·ªÅ: {img_info['title']}")
                                st.write(f"- ƒê·ªô li√™n quan: {img_info['relevance_score']:.2f}")
                                st.write("---")

                else:
                    st.subheader("üìù C√¢u tr·∫£ l·ªùi:")
                    fallback_answer = handle_no_results_fallback(question, config)
                    st.markdown(fallback_answer)

            except Exception as e:
                st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")
                logger.error(f"L·ªói: {e}")

                st.info("üîÑ ƒêang th·ª≠ ph∆∞∆°ng ph√°p d·ª± ph√≤ng...")
                try:
                    emergency_answer = handle_no_results_fallback(question, config)
                    st.subheader("üìù C√¢u tr·∫£ l·ªùi (D·ª± ph√≤ng):")
                    st.markdown(emergency_answer)
                except Exception as e2:
                    st.error(f"‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi: {e2}")

    st.markdown("---")
    st.markdown("ü§ñ **RAG Local System** - Tr·∫£ l·ªùi d·ª±a tr√™n database ho·∫∑c ki·∫øn th·ª©c chung")

if __name__ == "__main__":
    main()
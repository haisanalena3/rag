import os
import json
import streamlit as st
from pathlib import Path
from PIL import Image
import base64
from io import BytesIO
import traceback
import numpy as np
import re
import logging
import time
import shutil
import requests
import hashlib
from datetime import datetime
import asyncio
import nest_asyncio

from config_local import load_config_local
from rag_local import (
    load_documents, search_documents, ask_local_model,
    search_documents_with_threshold, adaptive_threshold_search,
    enhanced_search_with_metadata, get_vector_database
)
from collect_local import WebScraperLocal
from text_collector import TextContentCollector
from local_gemma import LocalGemmaClient
from mcp_client import MCPClient

# √Åp d·ª•ng nest_asyncio ƒë·ªÉ h·ªó tr·ª£ asyncio trong Streamlit
nest_asyncio.apply()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def validate_query_input(question, config):
    """Validate input query c∆° b·∫£n"""
    validation_config = config.get('QUERY_VALIDATION', {})
    min_length = validation_config.get('min_query_length', 1)
    
    if len(question.strip()) < min_length:
        return False, f"C√¢u h·ªèi qu√° ng·∫Øn. Vui l√≤ng nh·∫≠p √≠t nh·∫•t {min_length} k√Ω t·ª±."
    
    return True, "Query h·ª£p l·ªá"

class EnhancedMultimodalRAGLocal:
    def __init__(self, db_dir):
        self.db_dir = Path(db_dir)
        self.documents = []
        self.has_data = False
        self.query_history = []
        self.load_multimodal_data()

    def load_multimodal_data(self):
        """Load d·ªØ li·ªáu ƒëa ph∆∞∆°ng ti·ªán t·ª´ database"""
        self.documents = []
        logger.info(f"Ki·ªÉm tra th∆∞ m·ª•c db: {self.db_dir.absolute()}")
        
        if not self.db_dir.exists():
            logger.warning(f"Th∆∞ m·ª•c db kh√¥ng t·ªìn t·∫°i: {self.db_dir}")
            self.has_data = False
            return

        site_dirs = [d for d in self.db_dir.iterdir() if d.is_dir()]
        
        for site_dir in site_dirs:
            metadata_file = site_dir / "metadata.json"
            if not metadata_file.exists():
                continue
                
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                content_file = site_dir / "content.txt"
                text_content = ""
                if content_file.exists():
                    with open(content_file, 'r', encoding='utf-8') as f:
                        text_content = f.read()
                
                doc = {
                    'site_name': site_dir.name,
                    'url': data['metadata']['url'],
                    'title': data['metadata']['title'],
                    'description': data['metadata']['description'],
                    'text_content': text_content,
                    'images': data.get('images', []),
                    'image_dir': site_dir / "images",
                    'metadata': data['metadata']
                }
                
                self.documents.append(doc)
                
            except Exception as e:
                logger.error(f"L·ªói load d·ªØ li·ªáu t·ª´ {site_dir}: {e}")
        
        self.has_data = len(self.documents) > 0

    def get_database_info(self):
        """L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ database"""
        db_info = {
            'total_documents': len(self.documents),
            'total_words': 0,
            'total_images': 0,
            'content_types': {},
            'keywords': set(),
            'titles': [],
            'urls': [],
            'sample_content': []
        }
        
        for doc in self.documents:
            word_count = doc['metadata'].get('word_count', 0)
            db_info['total_words'] += word_count
            db_info['total_images'] += len(doc['images'])
            
            content_type = doc['metadata'].get('content_type', 'unknown')
            db_info['content_types'][content_type] = db_info['content_types'].get(content_type, 0) + 1
            
            db_info['titles'].append(doc['title'])
            db_info['urls'].append(doc['url'])
            
            if doc['text_content']:
                sample = doc['text_content'][:200] + "..."
                db_info['sample_content'].append({
                    'title': doc['title'],
                    'sample': sample
                })
        
        return db_info

    def enhanced_search_multimodal(self, query, threshold=None, top_k=3):
        """T√¨m ki·∫øm v·ªõi threshold t·ª´ config"""
        if not self.has_data:
            return [], 0
        
        config = load_config_local()
        threshold = threshold or config.get('DB_THRESHOLD', 0.3)
        
        relevant_docs, max_similarity = enhanced_search_with_metadata(
            query, self.documents, threshold, top_k
        )
        
        if not relevant_docs and threshold > 0.2:
            logger.info(f"Kh√¥ng t√¨m th·∫•y v·ªõi threshold {threshold}, th·ª≠ v·ªõi 0.2")
            relevant_docs, max_similarity = enhanced_search_with_metadata(
                query, self.documents, 0.2, top_k
            )
        
        self.query_history.append({
            'query': query,
            'similarity': max_similarity,
            'results_count': len(relevant_docs),
            'timestamp': time.time(),
            'threshold_used': threshold
        })
        
        return relevant_docs, max_similarity

    def get_relevant_images_for_context(self, relevant_docs, query, max_images=5):
        """L·∫•y ·∫£nh li√™n quan v·ªõi scoring c·∫£i ti·∫øn"""
        relevant_images = []
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        for doc in relevant_docs:
            for img_info in doc['images']:
                relevance_score = 0
                
                alt_text = img_info.get('alt', '').lower()
                title_text = img_info.get('title', '').lower()
                
                alt_words = set(alt_text.split())
                title_words = set(title_text.split())
                
                alt_matches = len(query_words.intersection(alt_words))
                title_matches = len(query_words.intersection(title_words))
                
                relevance_score += (alt_matches * 2) + (title_matches * 2)
                
                for word in query_words:
                    if len(word) > 3:
                        if word in alt_text:
                            relevance_score += 1
                        if word in title_text:
                            relevance_score += 1
                
                if len(alt_text) > 20 or len(title_text) > 20:
                    relevance_score += 0.5
                
                img_path = Path(img_info.get('local_path', ''))
                if img_path.exists() and (relevance_score > 0 or (not alt_text and not title_text)):
                    relevant_images.append({
                        'path': img_path,
                        'alt': img_info.get('alt', ''),
                        'title': img_info.get('title', ''),
                        'url': img_info.get('url', ''),
                        'source_doc': doc,
                        'relevance_score': relevance_score
                    })
        
        unique_images = []
        seen_paths = set()
        for img in sorted(relevant_images, key=lambda x: x['relevance_score'], reverse=True):
            if img['path'] not in seen_paths:
                unique_images.append(img)
                seen_paths.add(img['path'])
        
        return unique_images[:max_images]

def get_available_models():
    """L·∫•y danh s√°ch models c√≥ s·∫µn t·ª´ Ollama server"""
    try:
        config = load_config_local()
        base_url = config.get('LOCAL_MODEL', {}).get('base_url', 'http://localhost:11434')
        
        local_client = LocalGemmaClient(base_url=base_url)
        available_models = local_client.get_available_models()
        
        if available_models:
            return available_models
        else:
            return ["qwen2.5:3b"]
            
    except Exception as e:
        logger.error(f"L·ªói l·∫•y danh s√°ch models: {e}")
        return ["qwen2.5:3b"]

def display_database_debug_info(rag_system):
    """Hi·ªÉn th·ªã th√¥ng tin debug chi ti·∫øt v·ªÅ database"""
    st.subheader("üîç Th√¥ng tin chi ti·∫øt Database")
    
    db_info = rag_system.get_database_info()
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Documents", db_info['total_documents'])
    with col2:
        st.metric("T·ªïng t·ª´", f"{db_info['total_words']:,}")
    with col3:
        st.metric("T·ªïng ·∫£nh", db_info['total_images'])
    with col4:
        st.metric("Lo·∫°i n·ªôi dung", len(db_info['content_types']))
    
    if db_info['content_types']:
        st.write("**Ph√¢n lo·∫°i n·ªôi dung:**")
        for content_type, count in db_info['content_types'].items():
            st.write(f"- {content_type}: {count} documents")
    
    with st.expander("üìã Danh s√°ch Documents", expanded=False):
        for i, title in enumerate(db_info['titles'], 1):
            st.write(f"{i}. **{title}**")
            if i-1 < len(db_info['urls']):
                st.caption(f"URL: {db_info['urls'][i-1]}")
    
    with st.expander("üìÑ M·∫´u n·ªôi dung", expanded=False):
        for sample in db_info['sample_content'][:3]:
            st.write(f"**{sample['title']}**")
            st.write(sample['sample'])
            st.write("---")

def get_server_info(config, selected_model=None):
    """L·∫•y th√¥ng tin server model API v·ªõi model ƒë∆∞·ª£c ch·ªçn"""
    server_info = {
        'status': 'unknown',
        'base_url': config.get('LOCAL_MODEL', {}).get('base_url', 'N/A'),
        'model': selected_model or config.get('LOCAL_MODEL', {}).get('model', 'N/A'),
        'connection_status': False,
        'response_time': None,
        'available_models': []
    }
    
    try:
        base_url = server_info['base_url']
        if not base_url or base_url == 'N/A':
            server_info['status'] = 'No URL configured'
            return server_info
        
        local_client = LocalGemmaClient(base_url=base_url, model=server_info['model'])
        
        start_time = time.time()
        connection_status = local_client.check_connection()
        response_time = time.time() - start_time
        
        server_info['connection_status'] = connection_status
        server_info['response_time'] = response_time
        
        if connection_status:
            available_models = local_client.get_available_models()
            server_info['available_models'] = available_models
            server_info['status'] = 'Connected'
            
            if selected_model and not any(selected_model in model for model in available_models):
                server_info['status'] = f'Model {selected_model} not found'
        else:
            server_info['status'] = 'Connection Failed'
            
    except Exception as e:
        server_info['status'] = f'Error: {str(e)}'
        logger.error(f"L·ªói khi l·∫•y th√¥ng tin server: {e}")
    
    return server_info

def display_server_info(selected_model=None):
    """Hi·ªÉn th·ªã th√¥ng tin server trong sidebar v·ªõi model ƒë∆∞·ª£c ch·ªçn"""
    config = load_config_local()
    server_info = get_server_info(config, selected_model)
    
    st.sidebar.header("üñ•Ô∏è Th√¥ng tin Server")
    
    status = server_info['status']
    if server_info['connection_status']:
        st.sidebar.success(f"‚úÖ {status}")
    elif 'Error' in status or 'Failed' in status or 'not found' in status:
        st.sidebar.error(f"‚ùå {status}")
    else:
        st.sidebar.warning(f"‚ö†Ô∏è {status}")
    
    with st.sidebar.expander("Chi ti·∫øt Server", expanded=False):
        st.write(f"**URL:** {server_info['base_url']}")
        if server_info['response_time']:
            st.write(f"**Th·ªùi gian ph·∫£n h·ªìi:** {server_info['response_time']:.3f}s")
        st.write(f"**Model hi·ªán t·∫°i:** {server_info['model']}")
        
        if server_info['available_models']:
            st.write("**Models c√≥ s·∫µn:**")
            for model in server_info['available_models']:
                if model == server_info['model']:
                    st.write(f"- ‚úÖ **{model}** (ƒëang s·ª≠ d·ª•ng)")
                else:
                    st.write(f"- {model}")

def display_api_monitor():
    """Hi·ªÉn th·ªã monitor API requests"""
    with st.sidebar.expander("üìä API Monitor", expanded=False):
        if 'api_requests' not in st.session_state:
            st.session_state.api_requests = []
        
        st.write("**L·ªãch s·ª≠ API Requests:**")
        if st.session_state.api_requests:
            for i, req in enumerate(reversed(st.session_state.api_requests[-5:]), 1):
                with st.container():
                    st.write(f"**Request {i}:**")
                    st.caption(f"‚è∞ {req['timestamp']}")
                    st.caption(f"üîÑ Status: {req['status']}")
                    st.caption(f"‚ö° Time: {req['duration']:.2f}s")
                    if req.get('error'):
                        st.error(f"‚ùå {req['error']}")
                    st.write("---")
        else:
            st.info("Ch∆∞a c√≥ requests n√†o")
        
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠", key="clear_api_history"):
            st.session_state.api_requests = []
            st.rerun()

def log_api_request(status, duration, error=None, prompt_details=None):
    """Log API request v√†o session state v·ªõi th√¥ng tin prompt"""
    if 'api_requests' not in st.session_state:
        st.session_state.api_requests = []
    
    request_log = {
        'timestamp': time.strftime('%H:%M:%S'),
        'status': status,
        'duration': duration,
        'error': error,
        'prompt_details': prompt_details or {}
    }
    
    st.session_state.api_requests.append(request_log)
    
    if len(st.session_state.api_requests) > 20:
        st.session_state.api_requests = st.session_state.api_requests[-20:]

def serialize_mcp_result(obj):
    """Chuy·ªÉn ƒë·ªïi TextContent ho·∫∑c c√°c ƒë·ªëi t∆∞·ª£ng kh√¥ng JSON-serializable th√†nh chu·ªói"""
    if hasattr(obj, 'text'):
        return str(obj.text)
    elif hasattr(obj, '__str__'):
        return str(obj)
    raise TypeError(f"Object of type {type(obj).__name__} is not JSON serializable")

def display_prompt_details(question, context, model_config, response=None, duration=None):
    """Hi·ªÉn th·ªã chi ti·∫øt prompt ƒë∆∞·ª£c g·ª≠i l√™n model"""
    with st.expander("üîç Chi ti·∫øt Prompt & Request", expanded=False):
        tab1, tab2, tab3, tab4 = st.tabs(["üì§ Request", "üìù Prompt", "‚öôÔ∏è Config", "üì• Response"])
        
        with tab1:
            st.subheader("Request Information")
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**üéØ Model Settings:**")
                config = load_config_local()
                st.write(f"- **Base URL:** `{config['LOCAL_MODEL']['base_url']}`")
                st.write(f"- **Model:** `{model_config['selected_model']}`")
                st.write(f"- **Max Tokens:** {model_config['max_tokens']}")
                st.write(f"- **Temperature:** {model_config['temperature']}")
                st.write(f"- **Top P:** {model_config['top_p']}")
                st.write(f"- **Timeout:** {model_config['timeout']}s")
                st.write(f"- **Streaming:** {model_config.get('enable_streaming', False)}")
            
            with col2:
                st.write("**üìä Request Stats:**")
                if duration:
                    st.write(f"- **Duration:** {duration:.2f}s")
                st.write(f"- **Question Length:** {len(question)} chars")
                st.write(f"- **Context Length:** {len(context)} chars")
                st.write(f"- **Total Prompt Length:** {len(context) + len(question)} chars")
                if response:
                    st.write(f"- **Response Length:** {len(response)} chars")
                    st.write(f"- **Response Words:** {len(response.split())}")
        
        with tab2:
            st.subheader("Full Prompt Sent to Model")
            full_prompt = create_intelligent_multimodal_context(
                context.replace(model_config['system_prompt'], '').strip(),
                [],
                question,
                model_config['system_prompt']
            )
            
            st.write("**ü§ñ System Prompt:**")
            st.code(model_config['system_prompt'], language="text")
            
            st.write("**‚ùì User Question:**")
            st.code(question, language="text")
            
            st.write("**üìö Context Provided:**")
            context_only = context.replace(model_config['system_prompt'], '').strip()
            if context_only:
                if len(context_only) > 2000:
                    st.code(context_only[:2000] + "\n\n... (truncated)", language="text")
                    st.caption(f"Full context: {len(context_only)} characters")
                else:
                    st.code(context_only, language="text")
            else:
                st.info("No additional context provided")
            
            st.write("**üîó Complete Prompt:**")
            if len(full_prompt) > 3000:
                st.code(full_prompt[:3000] + "\n\n... (truncated for display)", language="text")
                st.caption(f"Full prompt: {len(full_prompt)} characters")
            else:
                st.code(full_prompt, language="text")
        
        with tab3:
            st.subheader("Model Configuration")
            config_dict = {
                "model": model_config['selected_model'],
                "max_tokens": model_config['max_tokens'],
                "temperature": model_config['temperature'],
                "top_p": model_config['top_p'],
                "timeout": model_config['timeout'],
                "stream": model_config.get('enable_streaming', False)
            }
            
            st.write("**JSON Configuration:**")
            st.code(json.dumps(config_dict, indent=2), language="json")
            
            st.write("**Equivalent API Call:**")
            config = load_config_local()
            api_call = f"""curl -X POST "{config['LOCAL_MODEL']['base_url']}/api/generate" \\
  -H "Content-Type: application/json" \\
  -d '{{
    "model": "{model_config['selected_model']}",
    "prompt": "{{PROMPT_TEXT}}",
    "stream": {str(model_config.get('enable_streaming', False)).lower()},
    "options": {{
      "temperature": {model_config['temperature']},
      "top_p": {model_config['top_p']},
      "num_predict": {model_config['max_tokens']}
    }}
  }}'"""
            st.code(api_call, language="bash")
        
        with tab4:
            if response:
                st.subheader("Model Response")
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**üìà Response Metrics:**")
                    st.write(f"- **Length:** {len(response)} characters")
                    st.write(f"- **Words:** {len(response.split())}")
                    st.write(f"- **Lines:** {len(response.split('\n'))}")
                    if duration:
                        words_per_sec = len(response.split()) / duration if duration > 0 else 0
                        st.write(f"- **Speed:** {words_per_sec:.1f} words/sec")
                
                with col2:
                    st.write("**üéØ Response Quality:**")
                    has_vietnamese = bool(re.search(r'[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]', response))
                    has_structure = bool(re.search(r'(\n\n|\d+\.|‚Ä¢|-)', response))
                    has_images = bool(re.search(r'\[IMAGE_\d+\]', response))
                    
                    st.write(f"- **Vietnamese:** {'‚úÖ' if has_vietnamese else '‚ùå'}")
                    st.write(f"- **Structured:** {'‚úÖ' if has_structure else '‚ùå'}")
                    st.write(f"- **Has Images:** {'‚úÖ' if has_images else '‚ùå'}")
                
                st.write("**üìù Full Response:**")
                st.code(response, language="text")
            else:
                st.info("No response available yet")

def create_sidebar():
    """T·∫°o sidebar v·ªõi c√°c ch·ª©c nƒÉng ƒëi·ªÅu khi·ªÉn"""
    st.sidebar.title("üîß ƒêi·ªÅu khi·ªÉn h·ªá th·ªëng")
    
    st.sidebar.header("ü§ñ Ch·ªçn Model")
    
    available_models = get_available_models()
    
    config = load_config_local()
    default_model = config.get('LOCAL_MODEL', {}).get('model', 'qwen2.5:3b')
    
    if default_model not in available_models:
        available_models.insert(0, default_model)
    
    selected_model = st.sidebar.selectbox(
        "Ch·ªçn Model:",
        options=available_models,
        index=0,
        help="Ch·ªçn model AI ƒë·ªÉ s·ª≠ d·ª•ng cho vi·ªác tr·∫£ l·ªùi c√¢u h·ªèi",
        key="model_selector"
    )
    
    if selected_model:
        try:
            local_client = LocalGemmaClient(
                base_url=config['LOCAL_MODEL']['base_url'],
                model=selected_model
            )
            model_exists = local_client.check_model_exists(selected_model)
            
            if model_exists:
                st.sidebar.success(f"‚úÖ Model {selected_model} s·∫µn s√†ng")
            else:
                st.sidebar.error(f"‚ùå Model {selected_model} kh√¥ng t·ªìn t·∫°i")
                
                if st.sidebar.button(f"üì• T·∫£i model {selected_model}", key="download_model"):
                    with st.sidebar.spinner(f"ƒêang t·∫£i {selected_model}..."):
                        success = local_client.pull_model(selected_model)
                        if success:
                            st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i {selected_model} th√†nh c√¥ng!")
                            st.rerun()
                        else:
                            st.sidebar.error(f"‚ùå Kh√¥ng th·ªÉ t·∫£i {selected_model}")
                            
        except Exception as e:
            st.sidebar.error(f"‚ùå L·ªói ki·ªÉm tra model: {e}")
    
    display_server_info(selected_model)
    
    display_api_monitor()
    
    with st.sidebar.expander("‚öôÔ∏è C·∫•u h√¨nh Model", expanded=False):
        max_tokens = st.slider(
            "Max Tokens:",
            min_value=100,
            max_value=4000,
            value=config.get('LOCAL_MODEL', {}).get('max_tokens', 2000),
            step=100,
            help="S·ªë token t·ªëi ƒëa cho response",
            key="max_tokens_slider"
        )
        
        temperature = st.slider(
            "Temperature:",
            min_value=0.0,
            max_value=2.0,
            value=config.get('LOCAL_MODEL', {}).get('temperature', 0.3),
            step=0.1,
            help="ƒê·ªô s√°ng t·∫°o c·ªßa model (0 = deterministic, 2 = very creative)",
            key="temperature_slider"
        )
        
        top_p = st.slider(
            "Top P:",
            min_value=0.1,
            max_value=1.0,
            value=config.get('LOCAL_MODEL', {}).get('top_p', 0.4),
            step=0.1,
            help="Nucleus sampling parameter",
            key="top_p_slider"
        )
        
        timeout = st.slider(
            "Timeout (seconds):",
            min_value=30,
            max_value=300,
            value=config.get('LOCAL_MODEL', {}).get('timeout', 180),
            step=30,
            help="Th·ªùi gian ch·ªù t·ªëi ƒëa cho response",
            key="timeout_slider"
        )
        
        enable_streaming = st.checkbox(
            "üåä B·∫≠t Streaming",
            value=True,
            help="Hi·ªÉn th·ªã response theo th·ªùi gian th·ª±c"
        )
        
        show_prompt_details = st.checkbox(
            "üîç Hi·ªÉn th·ªã chi ti·∫øt Prompt",
            value=True,
            help="Hi·ªÉn th·ªã prompt ƒë·∫ßy ƒë·ªß ƒë∆∞·ª£c g·ª≠i l√™n model"
        )
    
    with st.sidebar.expander("üìù System Prompt", expanded=False):
        default_system_prompt = """"""
        
        system_prompt = st.text_area(
            "Custom System Prompt:",
            value=default_system_prompt,
            height=200,
            help="T√πy ch·ªânh system prompt cho model",
            key="system_prompt_textarea"
        )
        
        st.write("**Prompt Templates:**")
        if st.button("üìö Academic", key="academic_prompt"):
            st.session_state.system_prompt_textarea = """B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªçc thu·∫≠t chuy√™n nghi·ªáp. H√£y tr·∫£ l·ªùi v·ªõi phong c√°ch h·ªçc thu·∫≠t, c√≥ tr√≠ch d·∫´n v√† ph√¢n t√≠ch s√¢u."""
            st.rerun()
        
        if st.button("üíº Business", key="business_prompt"):
            st.session_state.system_prompt_textarea = """B·∫°n l√† m·ªôt c·ªë v·∫•n kinh doanh. H√£y tr·∫£ l·ªùi v·ªõi g√≥c nh√¨n th·ª±c t·∫ø, t·∫≠p trung v√†o gi·∫£i ph√°p v√† hi·ªáu qu·∫£."""
            st.rerun()
        
        if st.button("üéì Tutorial", key="tutorial_prompt"):
            st.session_state.system_prompt_textarea = """B·∫°n l√† m·ªôt gi·∫£ng vi√™n. H√£y gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc m·ªôt c√°ch d·ªÖ hi·ªÉu, c√≥ v√≠ d·ª• c·ª• th·ªÉ."""
            st.rerun()
    
    with st.sidebar.expander("üîç C·∫•u h√¨nh T√¨m ki·∫øm", expanded=False):
        search_threshold = st.slider(
            "Ng∆∞·ª°ng t√¨m ki·∫øm:",
            min_value=0.1,
            max_value=0.8,
            value=0.3,
            step=0.05,
            help="Ng∆∞·ª°ng ƒë·ªô t∆∞∆°ng t·ª± t·ªëi thi·ªÉu"
        )
        
        max_results = st.selectbox(
            "S·ªë k·∫øt qu·∫£ t·ªëi ƒëa:",
            options=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
            index=2,
            help="S·ªë documents t·ªëi ƒëa ƒë·ªÉ t√¨m ki·∫øm"
        )
        
        max_images = st.slider(
            "S·ªë ·∫£nh t·ªëi ƒëa:",
            min_value=1,
            max_value=10,
            value=5,
            help="S·ªë ·∫£nh t·ªëi ƒëa hi·ªÉn th·ªã"
        )
        
        enable_fallback = st.checkbox(
            "üîÑ B·∫≠t AI Fallback",
            value=True,
            help="S·ª≠ d·ª•ng AI khi kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£"
        )
    
    with st.sidebar.expander("üì• Thu th·∫≠p d·ªØ li·ªáu", expanded=False):
        st.write("**Thu th·∫≠p t·ª´ URL**")
        new_url = st.text_input(
            "Nh·∫≠p URL c·∫ßn thu th·∫≠p:",
            placeholder="https://example.com/article",
            key="new_url_input"
        )
        
        st.write("**Nh·∫≠p n·ªôi dung vƒÉn b·∫£n**")
        new_text = st.text_area(
            "Nh·∫≠p n·ªôi dung ƒë·ªÉ th√™m v√†o database:",
            placeholder="Nh·∫≠p vƒÉn b·∫£n t·∫°i ƒë√¢y...",
            height=150,
            key="new_text_input"
        )
        
        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("üöÄ Thu th·∫≠p URL", key="collect_single", type="primary"):
                if new_url:
                    collect_single_url(new_url)
                else:
                    st.error("Vui l√≤ng nh·∫≠p URL")
        
        with col2:
            if st.button("üíæ L∆∞u n·ªôi dung", key="save_text"):
                if new_text.strip():
                    text_collector = TextContentCollector()
                    success = text_collector.save_text_content(new_text)
                    if success:
                        st.success("‚úÖ ƒê√£ l∆∞u n·ªôi dung v√†o database")
                        if 'rag_system' in st.session_state:
                            st.session_state.rag_system.load_multimodal_data()
                        st.rerun()
                    else:
                        st.error("‚ùå Kh√¥ng th·ªÉ l∆∞u n·ªôi dung")
                else:
                    st.error("Vui l√≤ng nh·∫≠p n·ªôi dung")
        
        with col3:
            if st.button("üóëÔ∏è X√≥a DB", key="clear_db"):
                clear_database()
    
    with st.sidebar.expander("üåê C·∫•u h√¨nh MCP Server", expanded=False):
        mcp_config = config.get('MCP', {})
        
        mcp_sse_url = st.text_input(
            "MCP SSE URL:",
            value=mcp_config.get('sse_url', 'http://localhost:8081/sse'),
            key="mcp_sse_url_input"
        )
        
        mcp_timeout = st.slider(
            "Timeout (seconds):",
            min_value=10,
            max_value=120,
            value=mcp_config.get('timeout', 30),
            step=10,
            key="mcp_timeout_slider"
        )
        
        if st.button("üîç Ki·ªÉm tra k·∫øt n·ªëi MCP", key="test_mcp_connection"):
            with st.spinner("ƒêang ki·ªÉm tra k·∫øt n·ªëi..."):
                mcp_client = MCPClient(mcp_sse_url, mcp_timeout)
                try:
                    success, message = asyncio.run(mcp_client.test_connection())
                    if success:
                        st.success(f"‚úÖ {message}")
                        st.session_state.mcp_client = mcp_client
                    else:
                        st.error(f"‚ùå {message}")
                except Exception as e:
                    st.error(f"‚ùå L·ªói k·∫øt n·ªëi: {str(e)}")
                finally:
                    if 'mcp_client' in locals() and mcp_client != st.session_state.get('mcp_client'):
                        mcp_client.close()

    return {
        'selected_model': selected_model,
        'max_tokens': max_tokens,
        'temperature': temperature,
        'top_p': top_p,
        'timeout': timeout,
        'system_prompt': system_prompt,
        'enable_streaming': enable_streaming,
        'search_threshold': search_threshold,
        'max_results': max_results,
        'max_images': max_images,
        'enable_fallback': enable_fallback,
        'show_prompt_details': show_prompt_details,
        'mcp_config': {
            'sse_url': mcp_sse_url,
            'timeout': mcp_timeout
        }
    }

def collect_single_url(url):
    """Thu th·∫≠p d·ªØ li·ªáu t·ª´ m·ªôt URL"""
    try:
        if not url.startswith(('http://', 'https://')):
            st.error("URL ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng http:// ho·∫∑c https://")
            return
        
        with st.spinner(f"ƒêang thu th·∫≠p t·ª´ {url}..."):
            scraper = WebScraperLocal(overwrite=False)
            success = scraper.fetch_and_save(url)
            
            if success:
                st.success(f"‚úÖ Thu th·∫≠p th√†nh c√¥ng t·ª´ {url}")
                if 'rag_system' in st.session_state:
                    st.session_state.rag_system.load_multimodal_data()
                st.rerun()
            else:
                st.error(f"‚ùå Kh√¥ng th·ªÉ thu th·∫≠p t·ª´ {url}")
                
    except Exception as e:
        st.error(f"L·ªói: {e}")
        logger.error(f"L·ªói thu th·∫≠p {url}: {e}")

def clear_database():
    """X√≥a database"""
    try:
        db_path = Path("db")
        if db_path.exists():
            shutil.rmtree(db_path)
            st.success("‚úÖ ƒê√£ x√≥a database")
            if 'rag_system' in st.session_state:
                st.session_state.rag_system.load_multimodal_data()
            st.rerun()
        else:
            st.warning("Database kh√¥ng t·ªìn t·∫°i")
    except Exception as e:
        st.error(f"L·ªói x√≥a database: {e}")

def handle_no_results_fallback(question, model_config):
    """X·ª≠ l√Ω fallback khi kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£"""
    if not model_config.get('enable_fallback', True):
        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan v√† AI fallback ƒë√£ b·ªã t·∫Øt."
    
    st.info("üîç Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong database ho·∫∑c MCP. ƒêang g·ªçi AI ƒë·ªÉ tr·∫£ l·ªùi...")
    
    fallback_context = f"""
{model_config['system_prompt']}

{question}
"""
    
    try:
        config = load_config_local()
        temp_config = config.copy()
        temp_config['LOCAL_MODEL'].update({
            'model': model_config['selected_model'],
            'max_tokens': model_config['max_tokens'],
            'temperature': model_config['temperature'],
            'top_p': model_config['top_p'],
            'timeout': model_config['timeout']
        })
        
        start_time = time.time()
        
        if model_config.get('enable_streaming', False):
            response = ask_local_model_streaming(question, fallback_context, temp_config)
        else:
            response = ask_local_model(question, fallback_context, temp_config)
        
        duration = time.time() - start_time
        
        prompt_details = {
            'type': 'fallback',
            'question': question,
            'context_length': len(fallback_context),
            'system_prompt_length': len(model_config['system_prompt']),
            'model': model_config['selected_model']
        }
        
        log_api_request("Success (Fallback)", duration, prompt_details=prompt_details)
        
        if model_config.get('show_prompt_details', True):
            display_prompt_details(question, fallback_context, model_config, response, duration)
        
        st.info("üí° **L∆∞u √Ω:** C√¢u tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa AI.")
        return response
        
    except Exception as e:
        duration = time.time() - start_time if 'start_time' in locals() else 0
        log_api_request("Error", duration, str(e))
        logger.error(f"Error in fallback AI response: {e}")
        return f"Xin l·ªói, kh√¥ng th·ªÉ tr·∫£ l·ªùi do l·ªói: {e}"

def ask_local_model_streaming(question, context, config):
    """H·ªèi model local v·ªõi streaming"""
    try:
        local_client = LocalGemmaClient(
            base_url=config["LOCAL_MODEL"]["base_url"],
            model=config["LOCAL_MODEL"]["model"]
        )
        
        response = local_client.generate_response(
            question,
            context,
            config["LOCAL_MODEL"]["max_tokens"],
            config["LOCAL_MODEL"]["temperature"],
            config["LOCAL_MODEL"]["top_p"],
            config["LOCAL_MODEL"]["timeout"]
        )
        
        return response
        
    except Exception as e:
        raise e

def create_intelligent_multimodal_context(text_context, relevant_images, question, system_prompt):
    """T·∫°o context ƒëa ph∆∞∆°ng ti·ªán th√¥ng minh"""
    context = f"{system_prompt}\n\nTh√¥ng tin vƒÉn b·∫£n:\n{text_context}\n\n"
    
    if relevant_images:
        context += "Th√¥ng tin h√¨nh ·∫£nh:\n"
        for i, img_info in enumerate(relevant_images, 1):
            context += f"\n[IMAGE_{i}]:"
            if img_info['alt']:
                context += f" M√¥ t·∫£: {img_info['alt']}"
            if img_info['title']:
                context += f" Ti√™u ƒë·ªÅ: {img_info['title']}"
            context += f" (Ngu·ªìn: {img_info['source_doc']['title']})"
    
    context += f"""

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi:
"""
    
    return context

def parse_answer_with_image_markers(answer, relevant_images):
    """Ph√¢n t√≠ch c√¢u tr·∫£ l·ªùi v√† t√°ch marker ·∫£nh"""
    image_pattern = r'\[IMAGE_(\d+)\]'
    parts = re.split(image_pattern, answer)
    
    content_parts = []
    for i, part in enumerate(parts):
        if part.isdigit():
            image_index = int(part) - 1
            if 0 <= image_index < len(relevant_images):
                content_parts.append({
                    'type': 'image',
                    'content': relevant_images[image_index],
                    'index': image_index
                })
        else:
            if part.strip():
                content_parts.append({
                    'type': 'text',
                    'content': part.strip()
                })
    
    return content_parts

def smart_display_answer_with_embedded_images(answer, relevant_images, enable_streaming=False):
    """Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi v·ªõi ·∫£nh ch√®n th√¥ng minh"""
    if not relevant_images:
        if enable_streaming:
            display_streaming_text(answer)
        else:
            st.markdown(answer)
        return
    
    content_parts = parse_answer_with_image_markers(answer, relevant_images)
    
    if not any(part['type'] == 'image' for part in content_parts):
        auto_embed_images_in_answer(answer, relevant_images, enable_streaming)
        return
    
    for part in content_parts:
        if part['type'] == 'text':
            if enable_streaming:
                display_streaming_text(part['content'])
            else:
                st.markdown(part['content'])
        elif part['type'] == 'image':
            img_info = part['content']
            try:
                image = Image.open(img_info['path'])
                caption = img_info.get('alt') or img_info.get('title') or f"H√¨nh ·∫£nh t·ª´ {img_info['source_doc']['title']}"
                
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    st.image(image, caption=caption, use_container_width=True)
                st.markdown("", unsafe_allow_html=True)
            except Exception as e:
                st.error(f"Kh√¥ng th·ªÉ hi·ªÉn th·ªã ·∫£nh: {e}")

def display_streaming_text(text):
    """Hi·ªÉn th·ªã text v·ªõi hi·ªáu ·ª©ng streaming"""
    placeholder = st.empty()
    displayed_text = ""
    
    for char in text:
        displayed_text += char
        placeholder.markdown(displayed_text)
        time.sleep(0.01)

def auto_embed_images_in_answer(answer, relevant_images, enable_streaming=False):
    """T·ª± ƒë·ªông ch√®n ·∫£nh v√†o c√¢u tr·∫£ l·ªùi"""
    paragraphs = [p.strip() for p in answer.split('\n\n') if p.strip()]
    total_images = len(relevant_images)
    
    if total_images == 0:
        if enable_streaming:
            display_streaming_text(answer)
        else:
            st.markdown(answer)
        return
    
    insert_positions = []
    if len(paragraphs) > 1:
        step = max(1, len(paragraphs) // (total_images + 1))
        for i in range(min(total_images, len(paragraphs) - 1)):
            pos = (i + 1) * step
            if pos < len(paragraphs):
                insert_positions.append(pos)
    
    image_index = 0
    for i, paragraph in enumerate(paragraphs):
        if enable_streaming:
            display_streaming_text(paragraph)
        else:
            st.markdown(paragraph)
        
        if i in insert_positions and image_index < len(relevant_images):
            img_info = relevant_images[image_index]
            try:
                image = Image.open(img_info['path'])
                caption = img_info.get('alt') or img_info.get('title') or "H√¨nh ·∫£nh minh h·ªça"
                
                st.markdown("", unsafe_allow_html=True)
                col1, col2, col3 = st.columns([1, 3, 1])
                with col2:
                    st.image(image, caption=caption, use_container_width=True)
                st.markdown("", unsafe_allow_html=True)
                
                image_index += 1
            except Exception as e:
                st.error(f"Kh√¥ng th·ªÉ hi·ªÉn th·ªã ·∫£nh: {e}")

def main():
    """H√†m ch√≠nh c·ªßa ·ª©ng d·ª•ng"""
    st.set_page_config(
        page_title="RAG Local v·ªõi ·∫¢nh Ch√®n & MCP SSE",
        page_icon="ü§ñ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    model_config = create_sidebar()
    
    st.title("ü§ñ DEMO AI")
    st.markdown("---")
    
    if 'rag_system' not in st.session_state:
        with st.spinner("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng..."):
            st.session_state.rag_system = EnhancedMultimodalRAGLocal("db")
    
    rag_system = st.session_state.rag_system
    
    if not rag_system.has_data:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng thu th·∫≠p d·ªØ li·ªáu t·ª´ sidebar.")
        
        col1, col2 = st.columns(2)
        with col1:
            st.info("üí° **H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:**")
            st.write("1. S·ª≠ d·ª•ng sidebar ƒë·ªÉ th√™m URL ho·∫∑c n·ªôi dung vƒÉn b·∫£n")
            st.write("2. C·∫•u h√¨nh MCP server qua SSE")
            st.write("3. Nh·∫•n 'Thu th·∫≠p' ho·∫∑c 'L∆∞u n·ªôi dung' ƒë·ªÉ t·∫£i d·ªØ li·ªáu")
            st.write("4. ƒê·∫∑t c√¢u h·ªèi sau khi c√≥ d·ªØ li·ªáu")
        
        with col2:
            st.info("üîß **T√≠nh nƒÉng:**")
            st.write("- Thu th·∫≠p t·ª´ URL")
            st.write("- Th√™m n·ªôi dung vƒÉn b·∫£n tr·ª±c ti·∫øp")
            st.write("- ·∫¢nh ch√®n th√¥ng minh")
            st.write("- AI tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi")
            st.write("- C·∫•u h√¨nh model linh ho·∫°t")
            st.write("- Streaming response")
            st.write("- API monitoring")
            st.write("- Chi ti·∫øt Prompt debugging")
            st.write("- Ch·ªçn model t·ª´ danh s√°ch")
            st.write("- K·∫øt n·ªëi MCP qua SSE")
    
    if rag_system.has_data:
        display_database_debug_info(rag_system)
    
    st.markdown("---")
    
    st.header("üí¨ H·ªèi ƒë√°p th√¥ng minh v·ªõi AI & MCP")
    
    st.info(f"ü§ñ **Model hi·ªán t·∫°i:** {model_config['selected_model']}")
    
    question = st.text_input(
        "ƒê·∫∑t c√¢u h·ªèi:",
        placeholder="B·∫°n c√≥ th·ªÉ h·ªèi b·∫•t k·ª≥ ƒëi·ªÅu g√¨...",
        help="Nh·∫≠p c√¢u h·ªèi v√† nh·∫•n Enter."
    )
    
    if question:
        config = load_config_local()
        is_valid, validation_message = validate_query_input(question, config)
        
        if not is_valid:
            st.error(f"‚ùå {validation_message}")
            return
        
        with st.spinner("üîç ƒêang t√¨m ki·∫øm v√† t·∫°o c√¢u tr·∫£ l·ªùi..."):
            try:
                start_time = time.time()
                
                enhanced_query = question
                mcp_result = None
                mcp_context = ""
                if 'mcp_client' in st.session_state:
                    mcp_client = st.session_state.mcp_client
                    try:
                        # L·∫•y prompt cho AI ch·ªçn tool
                        tool_prompt, _ = asyncio.run(mcp_client.process_query(question))
                        logger.info(f"MCP tool prompt: {tool_prompt}")
                        if tool_prompt and not tool_prompt.startswith("L·ªói"):
                            # G·ª≠i prompt ƒë·∫øn AI ƒë·ªÉ ch·ªçn tool
                            temp_config = config.copy()
                            temp_config['LOCAL_MODEL'].update({
                                'model': model_config['selected_model'],
                                'max_tokens': model_config['max_tokens'],
                                'temperature': model_config['temperature'],
                                'top_p': model_config['top_p'],
                                'timeout': model_config['timeout']
                            })
                            tool_selection = ask_local_model(tool_prompt, "", temp_config)
                            logger.info(f"AI tool selection: {tool_selection}")
                            
                            try:
                                tool_selection = tool_selection.replace('```json', '')
                                tool_selection = tool_selection.replace('```', '')
                                tool_info = json.loads(tool_selection)
                                if tool_info and "tool_name" in tool_info:
                                    # G·ªçi tool ƒë∆∞·ª£c AI ch·ªçn
                                    tool_name = tool_info["tool_name"]
                                    tool_params = tool_info.get("parameters", {})
                                    mcp_result = asyncio.run(mcp_client.call_tool(tool_name, tool_params))
                                    if mcp_result:
                                        mcp_context = f"K·∫øt qu·∫£ t·ª´ c√¥ng c·ª• {tool_name}: {json.dumps(mcp_result, ensure_ascii=False, default=serialize_mcp_result)}"
                                        st.info(f"üîß **K·∫øt qu·∫£ MCP Tool {tool_name}:** {mcp_result}")
                                    else:
                                        st.warning(f"‚ö†Ô∏è C√¥ng c·ª• {tool_name} {tool_selection} kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
                                
                            except json.JSONDecodeError:
                                logger.error(f"AI tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON kh√¥ng h·ª£p l·ªá: {tool_selection}")
                                st.warning(f"‚ö†Ô∏è AI tr·∫£ v·ªÅ l·ª±a ch·ªçn c√¥ng c·ª• kh√¥ng h·ª£p l·ªá: {tool_selection}")
                                # Fallback cho c√¢u h·ªèi v·ªÅ bug
                                if "bug" in question.lower():
                                    tool_name = "totalBug"
                                    target = question.split()[0].capitalize() if question.split() else "Unknown"
                                    tool_params = {"member_name": target}
                                    mcp_result = asyncio.run(mcp_client.call_tool(tool_name, tool_params))
                                    if mcp_result:
                                        mcp_context = f"K·∫øt qu·∫£ t·ª´ c√¥ng c·ª• {tool_name}: {json.dumps(mcp_result, ensure_ascii=False, default=serialize_mcp_result)}"
                                        st.info(f"üîß **K·∫øt qu·∫£ MCP Tool {tool_name} (fallback):** {mcp_result}")
                                    else:
                                        st.warning(f"‚ö†Ô∏è Fallback c√¥ng c·ª• {tool_name} kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
                        else:
                            st.error(f"‚ùå Kh√¥ng th·ªÉ t·∫°o prompt cho MCP tool: {tool_prompt if tool_prompt else 'Kh√¥ng c√≥ prompt'}")
                    except Exception as e:
                        logger.warning(f"MCP tool processing failed: {e}")
                        st.error(f"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω MCP tool: {e}")
                        # Fallback cho c√¢u h·ªèi v·ªÅ bug
                        if "bug" in question.lower() and 'mcp_client' in st.session_state:
                            tool_name = "totalBug"
                            target = question.split()[0].capitalize() if question.split() else "Unknown"
                            tool_params = {"member_name": target}
                            mcp_result = asyncio.run(mcp_client.call_tool(tool_name, tool_params))
                            if mcp_result:
                                mcp_context = f"K·∫øt qu·∫£ t·ª´ c√¥ng c·ª• {tool_name}: {json.dumps(mcp_result, ensure_ascii=False, default=serialize_mcp_result)}"
                                st.info(f"üîß **K·∫øt qu·∫£ MCP Tool {tool_name} (fallback):** {mcp_result}")
                            else:
                                st.warning(f"‚ö†Ô∏è Fallback c√¥ng c·ª• {tool_name} kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£")
                
                # T√¨m ki·∫øm trong database v·ªõi enhanced_query
                relevant_docs, similarity = rag_system.enhanced_search_multimodal(
                    enhanced_query, model_config['search_threshold'], model_config['max_results']
                )
                
                st.subheader("üîç K·∫øt qu·∫£ t√¨m ki·∫øm")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ƒê·ªô t∆∞∆°ng t·ª± t·ªëi ƒëa", f"{similarity:.3f}")
                with col2:
                    st.metric("S·ªë documents", len(relevant_docs))
                with col3:
                    st.metric("Ng∆∞·ª°ng s·ª≠ d·ª•ng", f"{model_config['search_threshold']:.3f}")
                with col4:
                    st.metric("Ng∆∞·ª°ng t·ªëi thi·ªÉu", f"{config.get('MIN_SIMILARITY_THRESHOLD', 0.5):.3f}")
                
                if relevant_docs and similarity >= config.get('MIN_SIMILARITY_THRESHOLD', 0.5):
                    with st.expander("üìã Documents ƒë∆∞·ª£c t√¨m th·∫•y", expanded=True):
                        for i, doc in enumerate(relevant_docs, 1):
                            st.write(f"**{i}. {doc['title']}**")
                            st.caption(f"URL: {doc['url']}")
                            st.caption(f"Lo·∫°i: {doc['metadata'].get('content_type', 'unknown')}")
                            
                            snippet = doc['text_content'][:300] + "..."
                            highlighted_snippet = snippet
                            for word in enhanced_query.split():
                                if len(word) > 3:
                                    highlighted_snippet = highlighted_snippet.replace(
                                        word, f"**{word}**"
                                    )
                            
                            st.write(f"*ƒêo·∫°n tr√≠ch:* {highlighted_snippet}")
                            st.write("---")
                
                # T√≠ch h·ª£p k·∫øt qu·∫£ MCP v√† database v√†o context
                text_context = mcp_context
                if relevant_docs:
                    text_context += "\n\n" + "\n\n".join([
                        f"Ti√™u ƒë·ªÅ: {doc['title']}\nM√¥ t·∫£: {doc['description']}\nN·ªôi dung: {doc['text_content'][:1000]}..."
                        for doc in relevant_docs
                    ])
                
                relevant_images = rag_system.get_relevant_images_for_context(
                    relevant_docs, enhanced_query, model_config['max_images']
                )
                
                context = create_intelligent_multimodal_context(
                    text_context, relevant_images, question, model_config['system_prompt']
                )
                
                temp_config = config.copy()
                temp_config['LOCAL_MODEL'].update({
                    'model': model_config['selected_model'],
                    'max_tokens': model_config['max_tokens'],
                    'temperature': model_config['temperature'],
                    'top_p': model_config['top_p'],
                    'timeout': model_config['timeout']
                })
                
                if model_config.get('enable_streaming', False):
                    answer = ask_local_model_streaming(question, context, temp_config)
                else:
                    answer = ask_local_model(question, context, temp_config)
                
                duration = time.time() - start_time
                
                prompt_details = {
                    'type': 'database_search_with_mcp',
                    'question': question,
                    'context_length': len(context),
                    'system_prompt_length': len(model_config['system_prompt']),
                    'documents_found': len(relevant_docs),
                    'images_found': len(relevant_images),
                    'mcp_result': str(mcp_result) if mcp_result else None,
                    'model': model_config['selected_model']
                }
                
                log_api_request("Success", duration, prompt_details=prompt_details)
                
                st.subheader("üìù C√¢u tr·∫£ l·ªùi:")
                st.success("‚úÖ **D·ª±a tr√™n database v√† MCP**")
                
                smart_display_answer_with_embedded_images(
                    answer, relevant_images, model_config.get('enable_streaming', False)
                )
                
                if model_config.get('show_prompt_details', True):
                    display_prompt_details(question, context, model_config, answer, duration)
                
                if relevant_images:
                    with st.expander("üñºÔ∏è Th√¥ng tin ·∫£nh", expanded=False):
                        for i, img_info in enumerate(relevant_images, 1):
                            st.write(f"**·∫¢nh {i}:**")
                            st.write(f"- Ngu·ªìn: {img_info['source_doc']['title']}")
                            if img_info['alt']:
                                st.write(f"- M√¥ t·∫£: {img_info['alt']}")
                            if img_info['title']:
                                st.write(f"- Ti√™u ƒë·ªÅ: {img_info['title']}")
                            st.write(f"- ƒê·ªô li√™n quan: {img_info['relevance_score']:.2f}")
                            st.write("---")
            
            except Exception as e:
                duration = time.time() - start_time if 'start_time' in locals() else 0
                log_api_request("Error", duration, str(e))
                st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {e}")
                logger.error(f"L·ªói: {e}")
                
                st.info("üîÑ ƒêang th·ª≠ ph∆∞∆°ng ph√°p d·ª± ph√≤ng...")
                try:
                    emergency_answer = handle_no_results_fallback(question, model_config)
                    st.subheader("üìù C√¢u tr·∫£ l·ªùi (D·ª± ph√≤ng):")
                    
                    if model_config.get('enable_streaming', False):
                        display_streaming_text(emergency_answer)
                    else:
                        st.markdown(emergency_answer)
                        
                except Exception as e2:
                    st.error(f"‚ùå Kh√¥ng th·ªÉ tr·∫£ l·ªùi: {e2}")
    
    st.markdown("---")
    st.markdown("ü§ñ **RAG Local System v·ªõi MCP SSE** - Tr·∫£ l·ªùi d·ª±a tr√™n database, MCP, ho·∫∑c ki·∫øn th·ª©c chung")

if __name__ == "__main__":
    main()